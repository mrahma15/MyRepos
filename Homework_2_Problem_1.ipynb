{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_2_Problem_1.ipynb",
      "provenance": [],
      "mount_file_id": "1PxP7utJcbe9UCG__3rDgNZd05f89W9Yp",
      "authorship_tag": "ABX9TyOnsALTagYweiBbn9p9FBIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrahma15/MyRepos/blob/main/Homework_2_Problem_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BoaX5pNd5Gqt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd  \n",
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "Rc68c8ZhL2dM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing = pd.DataFrame(pd.read_csv(\"/content/drive/MyDrive/Google Colab data/Homework 1/Housing.csv\")) \n",
        "housing.head()"
      ],
      "metadata": {
        "id": "MxFeXklr5ngP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "5b3eedc9-b523-4e78-80f2-22de1ad6792f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-457103ec-732d-489b-9eb1-4170143dbbb4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>mainroad</th>\n",
              "      <th>guestroom</th>\n",
              "      <th>basement</th>\n",
              "      <th>hotwaterheating</th>\n",
              "      <th>airconditioning</th>\n",
              "      <th>parking</th>\n",
              "      <th>prefarea</th>\n",
              "      <th>furnishingstatus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13300000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12250000</td>\n",
              "      <td>8960</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12250000</td>\n",
              "      <td>9960</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>semi-furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12215000</td>\n",
              "      <td>7500</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11410000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-457103ec-732d-489b-9eb1-4170143dbbb4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-457103ec-732d-489b-9eb1-4170143dbbb4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-457103ec-732d-489b-9eb1-4170143dbbb4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      price  area  bedrooms  ...  parking  prefarea furnishingstatus\n",
              "0  13300000  7420         4  ...        2       yes        furnished\n",
              "1  12250000  8960         4  ...        3        no        furnished\n",
              "2  12250000  9960         3  ...        2       yes   semi-furnished\n",
              "3  12215000  7500         4  ...        3       yes        furnished\n",
              "4  11410000  7420         4  ...        2        no        furnished\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price'] \n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "housing[num_vars] = scaler.fit_transform(housing[num_vars])\n",
        "\n",
        "Scaled_data = housing[num_vars] \n",
        "Scaled_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9K5HrkzRMMJF",
        "outputId": "1845b87e-6f26-492f-f798-53864646476c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1d0d8c95-4587-4bc4-af52-078d4e5b6e65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>parking</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.046726</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>1.378217</td>\n",
              "      <td>1.517692</td>\n",
              "      <td>4.566365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.757010</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>5.405809</td>\n",
              "      <td>2.532024</td>\n",
              "      <td>2.679409</td>\n",
              "      <td>4.004484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.218232</td>\n",
              "      <td>0.047278</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>1.517692</td>\n",
              "      <td>4.004484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.083624</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>2.679409</td>\n",
              "      <td>3.985755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.046726</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>-0.570187</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>1.517692</td>\n",
              "      <td>3.554979</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d0d8c95-4587-4bc4-af52-078d4e5b6e65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d0d8c95-4587-4bc4-af52-078d4e5b6e65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d0d8c95-4587-4bc4-af52-078d4e5b6e65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       area  bedrooms  bathrooms   stories   parking     price\n",
              "0  1.046726  1.403419   1.421812  1.378217  1.517692  4.566365\n",
              "1  1.757010  1.403419   5.405809  2.532024  2.679409  4.004484\n",
              "2  2.218232  0.047278   1.421812  0.224410  1.517692  4.004484\n",
              "3  1.083624  1.403419   1.421812  0.224410  2.679409  3.985755\n",
              "4  1.046726  1.403419  -0.570187  0.224410  1.517692  3.554979"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_Scaled = Scaled_data.pop('price')\n",
        "X_Scaled = Scaled_data"
      ],
      "metadata": {
        "id": "dFp4DyDTQYZ2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Scaled.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q2TSisHFQhBd",
        "outputId": "c9505007-95f9-457e-ba3d-f2b0fe016f36"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-34243c67-44f3-40c4-8f9d-e538445f9266\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>parking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.046726</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>1.378217</td>\n",
              "      <td>1.517692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.757010</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>5.405809</td>\n",
              "      <td>2.532024</td>\n",
              "      <td>2.679409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.218232</td>\n",
              "      <td>0.047278</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>1.517692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.083624</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>2.679409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.046726</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>-0.570187</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>1.517692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34243c67-44f3-40c4-8f9d-e538445f9266')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34243c67-44f3-40c4-8f9d-e538445f9266 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34243c67-44f3-40c4-8f9d-e538445f9266');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       area  bedrooms  bathrooms   stories   parking\n",
              "0  1.046726  1.403419   1.421812  1.378217  1.517692\n",
              "1  1.757010  1.403419   5.405809  2.532024  2.679409\n",
              "2  2.218232  0.047278   1.421812  0.224410  1.517692\n",
              "3  1.083624  1.403419   1.421812  0.224410  2.679409\n",
              "4  1.046726  1.403419  -0.570187  0.224410  1.517692"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_Scaled.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJd8xgjXQkfp",
        "outputId": "45e37e9b-f9e3-4665-962d-080261e609cb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4.566365\n",
              "1    4.004484\n",
              "2    4.004484\n",
              "3    3.985755\n",
              "4    3.554979\n",
              "Name: price, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_Scaled = torch.tensor(y_Scaled.values,dtype=torch.float).unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "aDgS5ityQqci"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_Scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q56iFfBSUQ9f",
        "outputId": "0ee8a8e9-54d1-4678-f93c-c5cc5f683cfb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.5664e+00],\n",
              "        [ 4.0045e+00],\n",
              "        [ 4.0045e+00],\n",
              "        [ 3.9858e+00],\n",
              "        [ 3.5550e+00],\n",
              "        [ 3.2553e+00],\n",
              "        [ 2.8807e+00],\n",
              "        [ 2.8807e+00],\n",
              "        [ 2.7309e+00],\n",
              "        [ 2.6934e+00],\n",
              "        [ 2.6934e+00],\n",
              "        [ 2.6297e+00],\n",
              "        [ 2.4312e+00],\n",
              "        [ 2.3938e+00],\n",
              "        [ 2.3938e+00],\n",
              "        [ 2.3188e+00],\n",
              "        [ 2.3188e+00],\n",
              "        [ 2.2439e+00],\n",
              "        [ 2.2065e+00],\n",
              "        [ 2.1877e+00],\n",
              "        [ 2.1315e+00],\n",
              "        [ 2.0941e+00],\n",
              "        [ 2.0754e+00],\n",
              "        [ 2.0754e+00],\n",
              "        [ 2.0379e+00],\n",
              "        [ 2.0192e+00],\n",
              "        [ 1.9780e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.8881e+00],\n",
              "        [ 1.8319e+00],\n",
              "        [ 1.7944e+00],\n",
              "        [ 1.7735e+00],\n",
              "        [ 1.7532e+00],\n",
              "        [ 1.7195e+00],\n",
              "        [ 1.7101e+00],\n",
              "        [ 1.6820e+00],\n",
              "        [ 1.6633e+00],\n",
              "        [ 1.6446e+00],\n",
              "        [ 1.5697e+00],\n",
              "        [ 1.5697e+00],\n",
              "        [ 1.4947e+00],\n",
              "        [ 1.4947e+00],\n",
              "        [ 1.4760e+00],\n",
              "        [ 1.4573e+00],\n",
              "        [ 1.4386e+00],\n",
              "        [ 1.4198e+00],\n",
              "        [ 1.4198e+00],\n",
              "        [ 1.4198e+00],\n",
              "        [ 1.3824e+00],\n",
              "        [ 1.3824e+00],\n",
              "        [ 1.3824e+00],\n",
              "        [ 1.3824e+00],\n",
              "        [ 1.3786e+00],\n",
              "        [ 1.3262e+00],\n",
              "        [ 1.3075e+00],\n",
              "        [ 1.3075e+00],\n",
              "        [ 1.2700e+00],\n",
              "        [ 1.2325e+00],\n",
              "        [ 1.2325e+00],\n",
              "        [ 1.2138e+00],\n",
              "        [ 1.1951e+00],\n",
              "        [ 1.1576e+00],\n",
              "        [ 1.1576e+00],\n",
              "        [ 1.1389e+00],\n",
              "        [ 1.1202e+00],\n",
              "        [ 1.0827e+00],\n",
              "        [ 1.0827e+00],\n",
              "        [ 1.0640e+00],\n",
              "        [ 1.0452e+00],\n",
              "        [ 1.0265e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 9.9655e-01],\n",
              "        [ 9.8906e-01],\n",
              "        [ 9.8906e-01],\n",
              "        [ 9.7033e-01],\n",
              "        [ 9.3287e-01],\n",
              "        [ 9.3287e-01],\n",
              "        [ 9.3287e-01],\n",
              "        [ 9.1414e-01],\n",
              "        [ 9.1414e-01],\n",
              "        [ 8.9541e-01],\n",
              "        [ 8.9541e-01],\n",
              "        [ 8.8417e-01],\n",
              "        [ 8.7668e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.1675e-01],\n",
              "        [ 8.0176e-01],\n",
              "        [ 7.8303e-01],\n",
              "        [ 7.8303e-01],\n",
              "        [ 7.6430e-01],\n",
              "        [ 7.6430e-01],\n",
              "        [ 7.6430e-01],\n",
              "        [ 7.4557e-01],\n",
              "        [ 7.4557e-01],\n",
              "        [ 7.2684e-01],\n",
              "        [ 7.1748e-01],\n",
              "        [ 7.0812e-01],\n",
              "        [ 7.0812e-01],\n",
              "        [ 7.0812e-01],\n",
              "        [ 7.0437e-01],\n",
              "        [ 7.0437e-01],\n",
              "        [ 6.7066e-01],\n",
              "        [ 6.7066e-01],\n",
              "        [ 6.7066e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.2945e-01],\n",
              "        [ 5.9574e-01],\n",
              "        [ 5.9574e-01],\n",
              "        [ 5.9199e-01],\n",
              "        [ 5.9199e-01],\n",
              "        [ 5.8825e-01],\n",
              "        [ 5.5828e-01],\n",
              "        [ 5.5828e-01],\n",
              "        [ 5.5828e-01],\n",
              "        [ 5.5453e-01],\n",
              "        [ 5.3955e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 4.7400e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.2717e-01],\n",
              "        [ 4.2717e-01],\n",
              "        [ 4.0845e-01],\n",
              "        [ 4.0845e-01],\n",
              "        [ 4.0845e-01],\n",
              "        [ 4.0470e-01],\n",
              "        [ 3.8972e-01],\n",
              "        [ 3.8972e-01],\n",
              "        [ 3.7099e-01],\n",
              "        [ 3.7099e-01],\n",
              "        [ 3.7099e-01],\n",
              "        [ 3.7099e-01],\n",
              "        [ 3.5226e-01],\n",
              "        [ 3.3353e-01],\n",
              "        [ 3.2978e-01],\n",
              "        [ 2.9607e-01],\n",
              "        [ 2.7734e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5486e-01],\n",
              "        [ 2.4737e-01],\n",
              "        [ 2.3988e-01],\n",
              "        [ 2.3988e-01],\n",
              "        [ 2.3988e-01],\n",
              "        [ 2.0242e-01],\n",
              "        [ 2.0242e-01],\n",
              "        [ 1.8369e-01],\n",
              "        [ 1.8369e-01],\n",
              "        [ 1.8369e-01],\n",
              "        [ 1.8369e-01],\n",
              "        [ 1.6496e-01],\n",
              "        [ 1.4623e-01],\n",
              "        [ 1.4623e-01],\n",
              "        [ 1.4623e-01],\n",
              "        [ 1.4623e-01],\n",
              "        [ 1.4249e-01],\n",
              "        [ 1.2750e-01],\n",
              "        [ 1.0878e-01],\n",
              "        [ 1.0878e-01],\n",
              "        [ 1.0128e-01],\n",
              "        [ 9.0046e-02],\n",
              "        [ 7.5062e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 6.7571e-02],\n",
              "        [ 6.7571e-02],\n",
              "        [ 5.2587e-02],\n",
              "        [ 3.3858e-02],\n",
              "        [ 3.3858e-02],\n",
              "        [ 3.3858e-02],\n",
              "        [ 3.3858e-02],\n",
              "        [ 1.5128e-02],\n",
              "        [ 1.5128e-02],\n",
              "        [ 1.4489e-04],\n",
              "        [-3.6010e-03],\n",
              "        [-3.6010e-03],\n",
              "        [-3.6010e-03],\n",
              "        [-7.3469e-03],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-5.9789e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-8.2264e-02],\n",
              "        [-9.7248e-02],\n",
              "        [-9.7248e-02],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1972e-01],\n",
              "        [-1.1972e-01],\n",
              "        [-1.3471e-01],\n",
              "        [-1.3471e-01],\n",
              "        [-1.3471e-01],\n",
              "        [-1.3471e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5718e-01],\n",
              "        [-1.5718e-01],\n",
              "        [-1.5718e-01],\n",
              "        [-1.7217e-01],\n",
              "        [-1.9089e-01],\n",
              "        [-1.9089e-01],\n",
              "        [-1.9464e-01],\n",
              "        [-1.9464e-01],\n",
              "        [-1.9464e-01],\n",
              "        [-2.0588e-01],\n",
              "        [-2.0962e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.3959e-01],\n",
              "        [-2.4708e-01],\n",
              "        [-2.4708e-01],\n",
              "        [-2.6207e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.8454e-01],\n",
              "        [-2.8454e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0702e-01],\n",
              "        [-3.0702e-01],\n",
              "        [-3.2200e-01],\n",
              "        [-3.2200e-01],\n",
              "        [-3.2200e-01],\n",
              "        [-3.4073e-01],\n",
              "        [-3.4073e-01],\n",
              "        [-3.4448e-01],\n",
              "        [-3.5759e-01],\n",
              "        [-3.5946e-01],\n",
              "        [-3.5946e-01],\n",
              "        [-3.5946e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.9692e-01],\n",
              "        [-3.9692e-01],\n",
              "        [-3.9692e-01],\n",
              "        [-4.0628e-01],\n",
              "        [-4.0628e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.7184e-01],\n",
              "        [-4.7184e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9806e-01],\n",
              "        [-5.0929e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.3177e-01],\n",
              "        [-5.3177e-01],\n",
              "        [-5.3177e-01],\n",
              "        [-5.4675e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6923e-01],\n",
              "        [-5.6923e-01],\n",
              "        [-5.8421e-01],\n",
              "        [-5.8421e-01],\n",
              "        [-5.8421e-01],\n",
              "        [-5.8421e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0669e-01],\n",
              "        [-6.2167e-01],\n",
              "        [-6.2167e-01],\n",
              "        [-6.4040e-01],\n",
              "        [-6.4040e-01],\n",
              "        [-6.4040e-01],\n",
              "        [-6.4040e-01],\n",
              "        [-6.5913e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.8160e-01],\n",
              "        [-6.9659e-01],\n",
              "        [-6.9659e-01],\n",
              "        [-6.9659e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1906e-01],\n",
              "        [-7.3405e-01],\n",
              "        [-7.3405e-01],\n",
              "        [-7.3405e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5652e-01],\n",
              "        [-7.6776e-01],\n",
              "        [-7.7151e-01],\n",
              "        [-7.7151e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-8.0896e-01],\n",
              "        [-8.0896e-01],\n",
              "        [-8.2020e-01],\n",
              "        [-8.2769e-01],\n",
              "        [-8.2769e-01],\n",
              "        [-8.2769e-01],\n",
              "        [-8.2769e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6890e-01],\n",
              "        [-8.7639e-01],\n",
              "        [-8.8182e-01],\n",
              "        [-8.8388e-01],\n",
              "        [-8.8388e-01],\n",
              "        [-8.8388e-01],\n",
              "        [-8.9886e-01],\n",
              "        [-9.0261e-01],\n",
              "        [-9.0261e-01],\n",
              "        [-9.0261e-01],\n",
              "        [-9.0261e-01],\n",
              "        [-9.2134e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4382e-01],\n",
              "        [-9.5880e-01],\n",
              "        [-9.6629e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-1.0150e+00],\n",
              "        [-1.0150e+00],\n",
              "        [-1.0150e+00],\n",
              "        [-1.0150e+00],\n",
              "        [-1.0244e+00],\n",
              "        [-1.0337e+00],\n",
              "        [-1.0337e+00],\n",
              "        [-1.0337e+00],\n",
              "        [-1.0524e+00],\n",
              "        [-1.0524e+00],\n",
              "        [-1.0899e+00],\n",
              "        [-1.0899e+00],\n",
              "        [-1.1086e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1311e+00],\n",
              "        [-1.1311e+00],\n",
              "        [-1.1573e+00],\n",
              "        [-1.1648e+00],\n",
              "        [-1.1648e+00],\n",
              "        [-1.1648e+00],\n",
              "        [-1.2023e+00],\n",
              "        [-1.2023e+00],\n",
              "        [-1.2023e+00],\n",
              "        [-1.2210e+00],\n",
              "        [-1.2210e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2622e+00],\n",
              "        [-1.2772e+00],\n",
              "        [-1.2772e+00],\n",
              "        [-1.2772e+00],\n",
              "        [-1.2959e+00],\n",
              "        [-1.3147e+00],\n",
              "        [-1.3334e+00],\n",
              "        [-1.3334e+00],\n",
              "        [-1.3334e+00],\n",
              "        [-1.3521e+00],\n",
              "        [-1.3559e+00],\n",
              "        [-1.4083e+00],\n",
              "        [-1.4270e+00],\n",
              "        [-1.4270e+00],\n",
              "        [-1.4270e+00],\n",
              "        [-1.5020e+00],\n",
              "        [-1.5394e+00],\n",
              "        [-1.5394e+00],\n",
              "        [-1.5581e+00],\n",
              "        [-1.5769e+00],\n",
              "        [-1.6051e+00],\n",
              "        [-1.6143e+00],\n",
              "        [-1.6143e+00],\n",
              "        [-1.6143e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Scaled = torch.tensor(X_Scaled.values,dtype=torch.float).unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "D4WmxgOoTHHJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1ak0XjDUItm",
        "outputId": "eaa4281d-7cff-474e-9abc-a16803a86323"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.0467,  1.4034,  ...,  1.3782,  1.5177]],\n",
              "\n",
              "        [[ 1.7570,  1.4034,  ...,  2.5320,  2.6794]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-1.0334,  0.0473,  ..., -0.9294, -0.8057]],\n",
              "\n",
              "        [[-0.5998,  0.0473,  ...,  0.2244, -0.8057]]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = X_Scaled.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_indices, val_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_yQhfXGVApI",
        "outputId": "6ada3dc5-c823-46d5-85bc-7fb87f240440"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 77, 455, 389,  56, 297,  89, 214, 366, 190, 503, 539, 456, 487,\n",
              "         400, 406, 362, 453, 246,  43, 181,  30, 215, 521, 232, 118, 432,\n",
              "         459, 409, 304,  23, 264, 394,  53, 161, 355, 413, 227, 419, 112,\n",
              "         437,  75, 510, 356, 393,  11, 163, 320, 100, 136, 411, 375, 422,\n",
              "          93, 540, 133, 324, 500, 200,  42, 179, 351, 517, 249, 388, 431,\n",
              "         315, 262, 277,  64, 106, 263, 239,   0, 380,  52, 102, 450, 125,\n",
              "         435, 188, 486,   3, 301, 328, 523, 519, 491,  45, 421, 266, 405,\n",
              "         115, 282,  68,  41, 428,  87,  71, 383,  35, 184,  83, 302, 260,\n",
              "         238, 467, 402, 466, 323,  99, 538, 316, 504, 240, 344,   7, 384,\n",
              "         296, 311, 241, 349, 334, 354, 385, 242, 265, 458, 254, 485,  61,\n",
              "         124, 206,  73, 146, 331, 525, 284, 505, 361, 391, 524, 299, 196,\n",
              "         348, 272, 237, 253, 326, 204,  25, 113, 414, 248, 511, 329,  67,\n",
              "         508, 279, 452,  47, 484, 399, 513, 195, 386, 482, 543, 229, 446,\n",
              "         243, 268, 438, 153, 167, 465,  82, 144, 364,  17,  80, 183, 433,\n",
              "         496, 298, 187, 216, 233, 306,  19, 251, 247,  14, 313,  96, 131,\n",
              "         245, 152, 267,  26, 244, 448, 515, 457, 474,   1, 129, 201, 137,\n",
              "         207,  39, 468,  22, 287, 343, 397,  10, 489, 121, 416, 365, 210,\n",
              "         134,  27,  66, 317, 202, 369, 509, 542,  24,  88,  44, 398, 425,\n",
              "         104, 205, 407, 337,  51, 168,  85, 285, 291, 280, 122, 471, 295,\n",
              "         261, 418, 472,  37, 392, 174, 142,  70,  69,  12, 518, 119,  79,\n",
              "         275, 140,  33, 346, 138, 209, 490,  13,  59, 497, 501, 427, 544,\n",
              "         404, 126, 430,  49,   6, 135,   8, 154, 231, 336, 234, 520, 464,\n",
              "         194, 441, 347, 359,  18,  31, 368, 357, 382, 534, 120, 493, 353,\n",
              "         449, 373, 171,  21, 149,  65, 463, 478, 335,  60, 541, 290, 197,\n",
              "         445, 454, 235, 145, 322, 494, 377,  94, 444,  86, 439,  20, 410,\n",
              "         527,  63, 103, 160, 250,  62, 109, 192, 480,  95, 305, 283, 271,\n",
              "          50, 212,  48, 221, 116, 436, 350, 143, 345, 531, 128, 132, 157,\n",
              "          29, 340, 499, 319, 276, 224, 147, 372, 502, 483,  92, 426,  54,\n",
              "         420, 151, 376, 396,  58, 274, 401,  91, 308, 417, 273, 526, 256,\n",
              "          81, 475, 460, 528, 381,  34, 327, 180, 332, 318, 141, 217,  46,\n",
              "         330, 156, 203, 269, 374, 219, 186, 270, 222, 127, 300, 158, 379,\n",
              "         101,  38, 360, 211, 286, 473, 415, 150, 165, 535, 292, 514,  97,\n",
              "         423,   2,  76, 225, 529, 530, 451, 236, 198, 476, 185, 139,  28,\n",
              "         289, 443, 537,   9,  90, 258, 178]),\n",
              " tensor([342, 110, 447, 130, 177, 462, 429,  84, 536, 123, 434, 293,  98,\n",
              "         259, 325,  57, 193,  55, 371, 512,  74, 255, 281, 107, 341, 403,\n",
              "         148, 477, 228, 424,  15, 199, 166, 516, 176, 363, 314, 294, 495,\n",
              "         288, 312,  36,  72, 469, 442, 159, 481, 367, 303, 387, 220, 208,\n",
              "         108, 370, 223, 352, 169, 338, 218, 175, 408, 173, 333, 111, 461,\n",
              "         164, 492, 230, 226, 412, 105,  16, 479, 155, 114, 507, 470, 252,\n",
              "         506, 358, 522, 307, 117,  32,   4, 498, 310, 309, 533, 162, 395,\n",
              "         172,   5, 170,  40, 488, 321, 257, 390, 182, 378, 532, 213, 339,\n",
              "         191, 278,  78, 440, 189]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = X_Scaled[train_indices]\n",
        "y_Train = y_Scaled[train_indices]\n",
        "\n",
        "X_val = X_Scaled[val_indices]\n",
        "y_val = y_Scaled[val_indices]"
      ],
      "metadata": {
        "id": "V5OVIEG3Viez"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "linear_model = nn.Linear(5, 1) # <1>\n",
        "linear_model(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzIFjLWYWEVc",
        "outputId": "cb269033-b19b-4273-fbd2-7c99779f3a5a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5922]],\n",
              "\n",
              "        [[-0.0707]],\n",
              "\n",
              "        [[ 0.2739]],\n",
              "\n",
              "        [[-0.3360]],\n",
              "\n",
              "        [[-0.5375]],\n",
              "\n",
              "        [[ 0.2127]],\n",
              "\n",
              "        [[-0.6092]],\n",
              "\n",
              "        [[ 0.5103]],\n",
              "\n",
              "        [[-0.7909]],\n",
              "\n",
              "        [[-0.8722]],\n",
              "\n",
              "        [[-0.4507]],\n",
              "\n",
              "        [[ 0.4049]],\n",
              "\n",
              "        [[-0.2328]],\n",
              "\n",
              "        [[ 0.0905]],\n",
              "\n",
              "        [[-0.3971]],\n",
              "\n",
              "        [[-0.6192]],\n",
              "\n",
              "        [[ 0.0643]],\n",
              "\n",
              "        [[-0.1287]],\n",
              "\n",
              "        [[-0.0146]],\n",
              "\n",
              "        [[ 0.4775]],\n",
              "\n",
              "        [[ 0.1874]],\n",
              "\n",
              "        [[ 0.1671]],\n",
              "\n",
              "        [[ 0.6682]],\n",
              "\n",
              "        [[-0.3511]],\n",
              "\n",
              "        [[ 1.0283]],\n",
              "\n",
              "        [[-1.6491]],\n",
              "\n",
              "        [[-0.3414]],\n",
              "\n",
              "        [[ 0.0385]],\n",
              "\n",
              "        [[ 0.5312]],\n",
              "\n",
              "        [[ 0.0612]],\n",
              "\n",
              "        [[-0.2511]],\n",
              "\n",
              "        [[ 0.1616]],\n",
              "\n",
              "        [[-0.2642]],\n",
              "\n",
              "        [[ 0.5936]],\n",
              "\n",
              "        [[-0.3803]],\n",
              "\n",
              "        [[ 0.2603]],\n",
              "\n",
              "        [[ 0.5876]],\n",
              "\n",
              "        [[-0.3369]],\n",
              "\n",
              "        [[ 0.1938]],\n",
              "\n",
              "        [[-0.2516]],\n",
              "\n",
              "        [[-0.1425]],\n",
              "\n",
              "        [[-0.0973]],\n",
              "\n",
              "        [[-0.0023]],\n",
              "\n",
              "        [[ 0.0965]],\n",
              "\n",
              "        [[ 0.6832]],\n",
              "\n",
              "        [[ 0.0773]],\n",
              "\n",
              "        [[ 0.4029]],\n",
              "\n",
              "        [[ 0.2529]],\n",
              "\n",
              "        [[-0.2876]],\n",
              "\n",
              "        [[-0.1226]],\n",
              "\n",
              "        [[-0.3436]],\n",
              "\n",
              "        [[ 0.2052]],\n",
              "\n",
              "        [[ 0.0392]],\n",
              "\n",
              "        [[ 0.7036]],\n",
              "\n",
              "        [[-0.1804]],\n",
              "\n",
              "        [[ 0.4537]],\n",
              "\n",
              "        [[-0.7465]],\n",
              "\n",
              "        [[ 0.3518]],\n",
              "\n",
              "        [[ 0.0618]],\n",
              "\n",
              "        [[-0.7166]],\n",
              "\n",
              "        [[ 0.1932]],\n",
              "\n",
              "        [[-0.6696]],\n",
              "\n",
              "        [[ 0.0773]],\n",
              "\n",
              "        [[-0.1103]],\n",
              "\n",
              "        [[ 0.0385]],\n",
              "\n",
              "        [[-0.0959]],\n",
              "\n",
              "        [[ 0.4115]],\n",
              "\n",
              "        [[ 0.0906]],\n",
              "\n",
              "        [[-0.0265]],\n",
              "\n",
              "        [[ 0.1402]],\n",
              "\n",
              "        [[ 0.0815]],\n",
              "\n",
              "        [[-0.4784]],\n",
              "\n",
              "        [[-0.4294]],\n",
              "\n",
              "        [[ 0.1572]],\n",
              "\n",
              "        [[ 0.2973]],\n",
              "\n",
              "        [[ 0.2577]],\n",
              "\n",
              "        [[-0.3805]],\n",
              "\n",
              "        [[-1.1519]],\n",
              "\n",
              "        [[ 0.3690]],\n",
              "\n",
              "        [[ 0.2771]],\n",
              "\n",
              "        [[ 0.1619]],\n",
              "\n",
              "        [[ 0.4587]],\n",
              "\n",
              "        [[-0.4358]],\n",
              "\n",
              "        [[-0.2010]],\n",
              "\n",
              "        [[-0.4801]],\n",
              "\n",
              "        [[ 0.6388]],\n",
              "\n",
              "        [[-0.5271]],\n",
              "\n",
              "        [[-0.5861]],\n",
              "\n",
              "        [[ 0.1740]],\n",
              "\n",
              "        [[-0.7561]],\n",
              "\n",
              "        [[-0.9424]],\n",
              "\n",
              "        [[-0.2379]],\n",
              "\n",
              "        [[ 0.0787]],\n",
              "\n",
              "        [[-0.3016]],\n",
              "\n",
              "        [[-0.4951]],\n",
              "\n",
              "        [[-0.5546]],\n",
              "\n",
              "        [[ 0.6785]],\n",
              "\n",
              "        [[-0.8923]],\n",
              "\n",
              "        [[ 0.3640]],\n",
              "\n",
              "        [[ 0.0112]],\n",
              "\n",
              "        [[ 0.3172]],\n",
              "\n",
              "        [[ 0.3545]],\n",
              "\n",
              "        [[-0.6204]],\n",
              "\n",
              "        [[-0.2047]],\n",
              "\n",
              "        [[-1.1642]],\n",
              "\n",
              "        [[ 0.5683]],\n",
              "\n",
              "        [[ 0.0744]],\n",
              "\n",
              "        [[-0.4261]],\n",
              "\n",
              "        [[ 0.2674]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtOavAuwXvp6",
        "outputId": "4ba54ade-b2b2-4402-e6d0-fd2af523be96"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.3496, -0.2951,  0.0739,  0.1067,  0.2391]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Kz-dsRXwOT",
        "outputId": "477bb3cd-b747-4d2d-9410-a704ccdea713"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.0446], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(5)\n",
        "linear_model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3MNPTx9X291",
        "outputId": "3239fc8f-170b-4d6d-fbab-1242addd05c8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.2697], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(10, 5)\n",
        "linear_model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDLfG6fRX-M8",
        "outputId": "bb67024a-0983-4234-947e-bc9db0f41b8c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2697],\n",
              "        [-0.2697],\n",
              "        [-0.2697],\n",
              "        [-0.2697],\n",
              "        [-0.2697],\n",
              "        [-0.2697],\n",
              "        [-0.2697],\n",
              "        [-0.2697],\n",
              "        [-0.2697],\n",
              "        [-0.2697]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = nn.Linear(5, 1) # <1>\n",
        "optimizer = optim.SGD(\n",
        "    linear_model.parameters(), # <2>\n",
        "    lr=1e-2)"
      ],
      "metadata": {
        "id": "Ifydd0dCYEAS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCSh-Ifedqjo",
        "outputId": "f2bea923-a0eb-4db8-8d0b-0c41b7820b4e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7fb9eb4eb050>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(linear_model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSoqnFh2drad",
        "outputId": "950d6afc-801e-4183-a93e-0eea3faef1ea"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.3876, -0.0527, -0.2747, -0.3256,  0.4338]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([0.0919], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, X_Train, X_val,\n",
        "                  y_Train, y_val):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        y_p_Train = model(X_Train) # <1>\n",
        "        loss_train = loss_fn(y_p_Train, y_Train)\n",
        "\n",
        "        t_p_val = model(X_val) # <1>\n",
        "        loss_val = loss_fn(y_p_Train, y_val)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward() # <2>\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch == 1 or epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
        "                  f\" Validation loss {loss_val.item():.4f}\")"
      ],
      "metadata": {
        "id": "qePIqhukduWM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_p_Train, y_Train):\n",
        "    squared_diffs = (y_p_Train - y_Train)**2\n",
        "    return squared_diffs.mean()\n",
        "\n",
        "linear_model = nn.Linear(5, 1) # <1>\n",
        "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = linear_model,\n",
        "    loss_fn = loss_fn,\n",
        "    X_Train = X_Train,\n",
        "    X_val = X_val, \n",
        "    y_Train = y_Train,\n",
        "    y_val = y_val)\n",
        "\n",
        "print()\n",
        "print(linear_model.weight)\n",
        "print(linear_model.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJafXhs1d1R_",
        "outputId": "656af8e3-3db6-472c-d2ee-4da131099657"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.5228, Validation loss 1.2975\n",
            "Epoch 20, Training loss 1.2408, Validation loss 1.0323\n",
            "Epoch 40, Training loss 1.1206, Validation loss 0.9237\n",
            "Epoch 60, Training loss 1.0720, Validation loss 0.8827\n",
            "Epoch 80, Training loss 1.0517, Validation loss 0.8673\n",
            "Epoch 100, Training loss 1.0430, Validation loss 0.8618\n",
            "Epoch 120, Training loss 1.0391, Validation loss 0.8600\n",
            "Epoch 140, Training loss 1.0372, Validation loss 0.8595\n",
            "Epoch 160, Training loss 1.0363, Validation loss 0.8595\n",
            "Epoch 180, Training loss 1.0358, Validation loss 0.8596\n",
            "Epoch 200, Training loss 1.0356, Validation loss 0.8598\n",
            "\n",
            "Parameter containing:\n",
            "tensor([[ 0.0016,  0.0025, -0.0153,  0.0144, -0.0005]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0102], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = nn.Linear(5, 1)\n",
        "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = linear_model,\n",
        "    loss_fn = nn.MSELoss(), # <1>\n",
        "    X_Train = X_Train,\n",
        "    X_val = X_val, \n",
        "    y_Train = y_Train,\n",
        "    y_val = y_val)\n",
        "\n",
        "print()\n",
        "print(linear_model.weight)\n",
        "print(linear_model.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3PHWXXtd53h",
        "outputId": "6d96543e-b32b-4f08-9e9d-790207423594"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.4417, Validation loss 1.3166\n",
            "Epoch 20, Training loss 1.2515, Validation loss 1.1114\n",
            "Epoch 40, Training loss 1.1523, Validation loss 1.0012\n",
            "Epoch 60, Training loss 1.1008, Validation loss 0.9422\n",
            "Epoch 80, Training loss 1.0729, Validation loss 0.9091\n",
            "Epoch 100, Training loss 1.0573, Validation loss 0.8900\n",
            "Epoch 120, Training loss 1.0483, Validation loss 0.8786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([436, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([109, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 140, Training loss 1.0431, Validation loss 0.8717\n",
            "Epoch 160, Training loss 1.0400, Validation loss 0.8675\n",
            "Epoch 180, Training loss 1.0381, Validation loss 0.8648\n",
            "Epoch 200, Training loss 1.0370, Validation loss 0.8632\n",
            "\n",
            "Parameter containing:\n",
            "tensor([[ 0.0061,  0.0394, -0.0331, -0.0034, -0.0070]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0248], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model = nn.Sequential(\n",
        "            nn.Linear(5, 8), # <1>\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(8, 1)) # <2>\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoj6arV9d_Ze",
        "outputId": "97e2c0ca-ff57-44cd-8074-5ad1152e48d1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[param.shape for param in seq_model.parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdMa4q40eESk",
        "outputId": "6f28497a-79ee-4a4b-9d38-0e7c501a0d17"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([8, 5]), torch.Size([8]), torch.Size([1, 8]), torch.Size([1])]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WphCE783eIUc",
        "outputId": "43859632-1702-4f2e-a281-066059026fe1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.weight torch.Size([8, 5])\n",
            "0.bias torch.Size([8])\n",
            "2.weight torch.Size([1, 8])\n",
            "2.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "seq_model = nn.Sequential(OrderedDict([\n",
        "    ('hidden_linear', nn.Linear(5, 8)),\n",
        "    ('hidden_activation', nn.Tanh()),\n",
        "    ('output_linear', nn.Linear(8, 1))\n",
        "]))\n",
        "\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihueAeyteUBh",
        "outputId": "f66b76a5-9a43-4888-9a41-41cc9db874af"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6I7nDgBeW1R",
        "outputId": "d45fa605-a4d8-43c0-b9e6-cadb61cdb3ad"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_linear.weight torch.Size([8, 5])\n",
            "hidden_linear.bias torch.Size([8])\n",
            "output_linear.weight torch.Size([1, 8])\n",
            "output_linear.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model.output_linear.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5npBO0uJebTO",
        "outputId": "4397f71d-08c9-4178-c101-b54ae8a145c6"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0.2726], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3) # <1>\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    X_Train = X_Train,\n",
        "    X_val = X_val, \n",
        "    y_Train = y_Train,\n",
        "    y_val = y_val)\n",
        "    \n",
        "print('output', seq_model(X_val))\n",
        "print('answer', y_val)\n",
        "print('hidden', seq_model.hidden_linear.weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFu6fNDRegDH",
        "outputId": "d9467fe7-a9cd-4d95-b7f7-697a15d145d4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.0688, Validation loss 0.8798\n",
            "Epoch 20, Training loss 1.0664, Validation loss 0.8784\n",
            "Epoch 40, Training loss 1.0642, Validation loss 0.8771\n",
            "Epoch 60, Training loss 1.0623, Validation loss 0.8759\n",
            "Epoch 80, Training loss 1.0605, Validation loss 0.8749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([436, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([109, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Training loss 1.0589, Validation loss 0.8740\n",
            "Epoch 120, Training loss 1.0575, Validation loss 0.8732\n",
            "Epoch 140, Training loss 1.0561, Validation loss 0.8725\n",
            "Epoch 160, Training loss 1.0550, Validation loss 0.8719\n",
            "Epoch 180, Training loss 1.0539, Validation loss 0.8714\n",
            "Epoch 200, Training loss 1.0529, Validation loss 0.8709\n",
            "output tensor([[[-0.1014]],\n",
            "\n",
            "        [[-0.2576]],\n",
            "\n",
            "        [[ 0.0251]],\n",
            "\n",
            "        [[-0.0724]],\n",
            "\n",
            "        [[-0.1128]],\n",
            "\n",
            "        [[ 0.0512]],\n",
            "\n",
            "        [[-0.0385]],\n",
            "\n",
            "        [[-0.1856]],\n",
            "\n",
            "        [[-0.0466]],\n",
            "\n",
            "        [[ 0.2480]],\n",
            "\n",
            "        [[-0.0291]],\n",
            "\n",
            "        [[-0.0971]],\n",
            "\n",
            "        [[ 0.2014]],\n",
            "\n",
            "        [[-0.0897]],\n",
            "\n",
            "        [[-0.0252]],\n",
            "\n",
            "        [[ 0.0935]],\n",
            "\n",
            "        [[-0.0146]],\n",
            "\n",
            "        [[-0.2010]],\n",
            "\n",
            "        [[-0.0028]],\n",
            "\n",
            "        [[ 0.0434]],\n",
            "\n",
            "        [[-0.1290]],\n",
            "\n",
            "        [[-0.0928]],\n",
            "\n",
            "        [[-0.1488]],\n",
            "\n",
            "        [[ 0.0061]],\n",
            "\n",
            "        [[-0.0564]],\n",
            "\n",
            "        [[-0.0056]],\n",
            "\n",
            "        [[ 0.0076]],\n",
            "\n",
            "        [[-0.0192]],\n",
            "\n",
            "        [[-0.1819]],\n",
            "\n",
            "        [[ 0.0153]],\n",
            "\n",
            "        [[-0.2979]],\n",
            "\n",
            "        [[-0.1364]],\n",
            "\n",
            "        [[-0.2437]],\n",
            "\n",
            "        [[-0.0932]],\n",
            "\n",
            "        [[-0.2294]],\n",
            "\n",
            "        [[ 0.0225]],\n",
            "\n",
            "        [[-0.0932]],\n",
            "\n",
            "        [[ 0.0713]],\n",
            "\n",
            "        [[-0.1272]],\n",
            "\n",
            "        [[-0.0550]],\n",
            "\n",
            "        [[-0.0211]],\n",
            "\n",
            "        [[-0.0080]],\n",
            "\n",
            "        [[ 0.1859]],\n",
            "\n",
            "        [[-0.0088]],\n",
            "\n",
            "        [[-0.0887]],\n",
            "\n",
            "        [[ 0.1254]],\n",
            "\n",
            "        [[ 0.0493]],\n",
            "\n",
            "        [[ 0.0211]],\n",
            "\n",
            "        [[-0.0604]],\n",
            "\n",
            "        [[-0.0277]],\n",
            "\n",
            "        [[-0.0387]],\n",
            "\n",
            "        [[ 0.1031]],\n",
            "\n",
            "        [[ 0.0285]],\n",
            "\n",
            "        [[-0.1548]],\n",
            "\n",
            "        [[-0.2080]],\n",
            "\n",
            "        [[-0.1107]],\n",
            "\n",
            "        [[ 0.2106]],\n",
            "\n",
            "        [[ 0.0187]],\n",
            "\n",
            "        [[-0.0151]],\n",
            "\n",
            "        [[-0.1446]],\n",
            "\n",
            "        [[ 0.0096]],\n",
            "\n",
            "        [[ 0.1629]],\n",
            "\n",
            "        [[ 0.0192]],\n",
            "\n",
            "        [[-0.1692]],\n",
            "\n",
            "        [[-0.0192]],\n",
            "\n",
            "        [[-0.0106]],\n",
            "\n",
            "        [[-0.0614]],\n",
            "\n",
            "        [[-0.0325]],\n",
            "\n",
            "        [[ 0.1818]],\n",
            "\n",
            "        [[ 0.0342]],\n",
            "\n",
            "        [[ 0.1996]],\n",
            "\n",
            "        [[ 0.0239]],\n",
            "\n",
            "        [[-0.0276]],\n",
            "\n",
            "        [[-0.1920]],\n",
            "\n",
            "        [[-0.0832]],\n",
            "\n",
            "        [[ 0.0220]],\n",
            "\n",
            "        [[-0.0827]],\n",
            "\n",
            "        [[-0.1089]],\n",
            "\n",
            "        [[ 0.0431]],\n",
            "\n",
            "        [[ 0.0257]],\n",
            "\n",
            "        [[ 0.0393]],\n",
            "\n",
            "        [[-0.1942]],\n",
            "\n",
            "        [[-0.0281]],\n",
            "\n",
            "        [[-0.0130]],\n",
            "\n",
            "        [[-0.3105]],\n",
            "\n",
            "        [[ 0.0711]],\n",
            "\n",
            "        [[-0.1111]],\n",
            "\n",
            "        [[-0.0374]],\n",
            "\n",
            "        [[ 0.0422]],\n",
            "\n",
            "        [[ 0.2137]],\n",
            "\n",
            "        [[-0.0968]],\n",
            "\n",
            "        [[-0.2319]],\n",
            "\n",
            "        [[ 0.0845]],\n",
            "\n",
            "        [[ 0.1617]],\n",
            "\n",
            "        [[-0.0929]],\n",
            "\n",
            "        [[ 0.0498]],\n",
            "\n",
            "        [[-0.1193]],\n",
            "\n",
            "        [[-0.1338]],\n",
            "\n",
            "        [[ 0.0716]],\n",
            "\n",
            "        [[ 0.0034]],\n",
            "\n",
            "        [[ 0.1227]],\n",
            "\n",
            "        [[ 0.0404]],\n",
            "\n",
            "        [[ 0.1509]],\n",
            "\n",
            "        [[ 0.0277]],\n",
            "\n",
            "        [[-0.0665]],\n",
            "\n",
            "        [[-0.1752]],\n",
            "\n",
            "        [[-0.2602]],\n",
            "\n",
            "        [[-0.0274]],\n",
            "\n",
            "        [[ 0.0239]]], grad_fn=<AddBackward0>)\n",
            "answer tensor([[-0.4906],\n",
            "        [ 0.7081],\n",
            "        [-0.8652],\n",
            "        [ 0.5882],\n",
            "        [ 0.2549],\n",
            "        [-0.9026],\n",
            "        [-0.7715],\n",
            "        [ 0.9329],\n",
            "        [-1.5020],\n",
            "        [ 0.6332],\n",
            "        [-0.7902],\n",
            "        [-0.3033],\n",
            "        [ 0.8167],\n",
            "        [-0.1572],\n",
            "        [-0.4063],\n",
            "        [ 1.3262],\n",
            "        [ 0.1425],\n",
            "        [ 1.3824],\n",
            "        [-0.6029],\n",
            "        [-1.2023],\n",
            "        [ 1.0078],\n",
            "        [-0.1534],\n",
            "        [-0.2658],\n",
            "        [ 0.7268],\n",
            "        [-0.4906],\n",
            "        [-0.6779],\n",
            "        [ 0.4459],\n",
            "        [-0.9775],\n",
            "        [-0.0411],\n",
            "        [-0.7528],\n",
            "        [ 2.3188],\n",
            "        [ 0.0751],\n",
            "        [ 0.2961],\n",
            "        [-1.2397],\n",
            "        [ 0.2586],\n",
            "        [-0.5655],\n",
            "        [-0.3595],\n",
            "        [-0.3033],\n",
            "        [-1.0899],\n",
            "        [-0.3033],\n",
            "        [-0.3576],\n",
            "        [ 1.7532],\n",
            "        [ 1.0452],\n",
            "        [-0.9401],\n",
            "        [-0.8277],\n",
            "        [ 0.3710],\n",
            "        [-0.9775],\n",
            "        [-0.5842],\n",
            "        [-0.3033],\n",
            "        [-0.6779],\n",
            "        [ 0.0151],\n",
            "        [ 0.0713],\n",
            "        [ 0.7175],\n",
            "        [-0.6029],\n",
            "        [-0.0036],\n",
            "        [-0.5280],\n",
            "        [ 0.2586],\n",
            "        [-0.4718],\n",
            "        [ 0.0339],\n",
            "        [ 0.2586],\n",
            "        [-0.7153],\n",
            "        [ 0.2586],\n",
            "        [-0.4531],\n",
            "        [ 0.7081],\n",
            "        [-0.9026],\n",
            "        [ 0.3335],\n",
            "        [-1.0524],\n",
            "        [-0.0411],\n",
            "        [-0.0411],\n",
            "        [-0.7153],\n",
            "        [ 0.7456],\n",
            "        [ 2.3188],\n",
            "        [-0.9775],\n",
            "        [ 0.4084],\n",
            "        [ 0.6707],\n",
            "        [-1.1648],\n",
            "        [-0.9401],\n",
            "        [-0.1347],\n",
            "        [-1.1573],\n",
            "        [-0.5468],\n",
            "        [-1.2772],\n",
            "        [-0.3220],\n",
            "        [ 0.6332],\n",
            "        [ 1.8881],\n",
            "        [ 3.5550],\n",
            "        [-1.1274],\n",
            "        [-0.3407],\n",
            "        [-0.3407],\n",
            "        [-1.4270],\n",
            "        [ 0.3710],\n",
            "        [-0.6779],\n",
            "        [ 0.2586],\n",
            "        [ 3.2553],\n",
            "        [ 0.2586],\n",
            "        [ 1.6633],\n",
            "        [-1.0244],\n",
            "        [-0.3969],\n",
            "        [-0.1534],\n",
            "        [-0.6779],\n",
            "        [ 0.2024],\n",
            "        [-0.6029],\n",
            "        [-1.4083],\n",
            "        [ 0.0676],\n",
            "        [-0.4718],\n",
            "        [ 0.1462],\n",
            "        [-0.2621],\n",
            "        [ 1.0078],\n",
            "        [-0.8202],\n",
            "        [ 0.1462]])\n",
            "hidden tensor([[-0.0002,  0.0026, -0.0134, -0.0063,  0.0122],\n",
            "        [-0.0073, -0.0016,  0.0108,  0.0111, -0.0225],\n",
            "        [-0.0092, -0.0019,  0.0167,  0.0128, -0.0285],\n",
            "        [-0.0123,  0.0030,  0.0283,  0.0252, -0.0364],\n",
            "        [-0.0044,  0.0005, -0.0179, -0.0119,  0.0209],\n",
            "        [ 0.0096,  0.0015, -0.0152, -0.0188,  0.0317],\n",
            "        [-0.0041,  0.0011,  0.0151,  0.0119, -0.0180],\n",
            "        [-0.0056,  0.0011,  0.0299,  0.0245, -0.0385]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 1b\n",
        "\n",
        "seq_model = nn.Sequential(OrderedDict([\n",
        "    ('hidden_linear_1', nn.Linear(5, 8)),\n",
        "    ('hidden1_activation', nn.Tanh()),\n",
        "    ('hidden_linear_2', nn.Linear(8, 4)),\n",
        "    ('hidden2_activation', nn.Tanh()),\n",
        "    ('hidden_linear_3', nn.Linear(4, 2)),\n",
        "    ('hidden3_activation', nn.Tanh()),\n",
        "    ('output_linear', nn.Linear(2, 1))\n",
        "]))\n",
        "\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd1DJ4Gnej86",
        "outputId": "1d005947-b2a2-497c-afd1-0661502f9101"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear_1): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden1_activation): Tanh()\n",
              "  (hidden_linear_2): Linear(in_features=8, out_features=4, bias=True)\n",
              "  (hidden2_activation): Tanh()\n",
              "  (hidden_linear_3): Linear(in_features=4, out_features=2, bias=True)\n",
              "  (hidden3_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=2, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voDPpsUTpB7S",
        "outputId": "a6ead3f8-14c2-4357-b86a-35cc31095ec7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_linear_1.weight torch.Size([8, 5])\n",
            "hidden_linear_1.bias torch.Size([8])\n",
            "hidden_linear_2.weight torch.Size([4, 8])\n",
            "hidden_linear_2.bias torch.Size([4])\n",
            "hidden_linear_3.weight torch.Size([2, 4])\n",
            "hidden_linear_3.bias torch.Size([2])\n",
            "output_linear.weight torch.Size([1, 2])\n",
            "output_linear.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model.output_linear.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnd1Sd6jpChJ",
        "outputId": "51d1b651-726c-4950-8080-e44ed2832374"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.2168], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3) # <1>\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    X_Train = X_Train,\n",
        "    X_val = X_val, \n",
        "    y_Train = y_Train,\n",
        "    y_val = y_val)\n",
        "    \n",
        "print('output', seq_model(X_val))\n",
        "print('answer', y_val)\n",
        "#print('hidden', seq_model.hidden_linear.weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znFhxQXLpGXw",
        "outputId": "8d4d1013-d278-45c4-9965-596d1f7316f3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.0745, Validation loss 0.8709\n",
            "Epoch 20, Training loss 1.0708, Validation loss 0.8686\n",
            "Epoch 40, Training loss 1.0673, Validation loss 0.8665\n",
            "Epoch 60, Training loss 1.0641, Validation loss 0.8647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([436, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([109, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Training loss 1.0613, Validation loss 0.8631\n",
            "Epoch 100, Training loss 1.0587, Validation loss 0.8618\n",
            "Epoch 120, Training loss 1.0564, Validation loss 0.8606\n",
            "Epoch 140, Training loss 1.0544, Validation loss 0.8596\n",
            "Epoch 160, Training loss 1.0525, Validation loss 0.8588\n",
            "Epoch 180, Training loss 1.0508, Validation loss 0.8581\n",
            "Epoch 200, Training loss 1.0493, Validation loss 0.8575\n",
            "output tensor([[[-0.1318]],\n",
            "\n",
            "        [[-0.1156]],\n",
            "\n",
            "        [[-0.1105]],\n",
            "\n",
            "        [[-0.1236]],\n",
            "\n",
            "        [[-0.1406]],\n",
            "\n",
            "        [[-0.0777]],\n",
            "\n",
            "        [[-0.1024]],\n",
            "\n",
            "        [[-0.0558]],\n",
            "\n",
            "        [[-0.0889]],\n",
            "\n",
            "        [[-0.1070]],\n",
            "\n",
            "        [[-0.0909]],\n",
            "\n",
            "        [[-0.1066]],\n",
            "\n",
            "        [[-0.0998]],\n",
            "\n",
            "        [[-0.1319]],\n",
            "\n",
            "        [[-0.0872]],\n",
            "\n",
            "        [[-0.0858]],\n",
            "\n",
            "        [[-0.1259]],\n",
            "\n",
            "        [[-0.0988]],\n",
            "\n",
            "        [[-0.0916]],\n",
            "\n",
            "        [[-0.0935]],\n",
            "\n",
            "        [[-0.0747]],\n",
            "\n",
            "        [[-0.1255]],\n",
            "\n",
            "        [[-0.0961]],\n",
            "\n",
            "        [[-0.1067]],\n",
            "\n",
            "        [[-0.0638]],\n",
            "\n",
            "        [[-0.2191]],\n",
            "\n",
            "        [[-0.1062]],\n",
            "\n",
            "        [[-0.1279]],\n",
            "\n",
            "        [[-0.0543]],\n",
            "\n",
            "        [[-0.0868]],\n",
            "\n",
            "        [[-0.0879]],\n",
            "\n",
            "        [[-0.0766]],\n",
            "\n",
            "        [[-0.1305]],\n",
            "\n",
            "        [[-0.0930]],\n",
            "\n",
            "        [[-0.1391]],\n",
            "\n",
            "        [[-0.1114]],\n",
            "\n",
            "        [[-0.0817]],\n",
            "\n",
            "        [[-0.0813]],\n",
            "\n",
            "        [[-0.0742]],\n",
            "\n",
            "        [[-0.1076]],\n",
            "\n",
            "        [[-0.1082]],\n",
            "\n",
            "        [[-0.0861]],\n",
            "\n",
            "        [[-0.0897]],\n",
            "\n",
            "        [[-0.1234]],\n",
            "\n",
            "        [[-0.0871]],\n",
            "\n",
            "        [[-0.0930]],\n",
            "\n",
            "        [[-0.1018]],\n",
            "\n",
            "        [[-0.1120]],\n",
            "\n",
            "        [[-0.1197]],\n",
            "\n",
            "        [[-0.0988]],\n",
            "\n",
            "        [[-0.0832]],\n",
            "\n",
            "        [[-0.0788]],\n",
            "\n",
            "        [[-0.0556]],\n",
            "\n",
            "        [[-0.0936]],\n",
            "\n",
            "        [[-0.1029]],\n",
            "\n",
            "        [[-0.1123]],\n",
            "\n",
            "        [[-0.1012]],\n",
            "\n",
            "        [[-0.1007]],\n",
            "\n",
            "        [[-0.1260]],\n",
            "\n",
            "        [[-0.1106]],\n",
            "\n",
            "        [[-0.1162]],\n",
            "\n",
            "        [[-0.1065]],\n",
            "\n",
            "        [[-0.0858]],\n",
            "\n",
            "        [[-0.0933]],\n",
            "\n",
            "        [[-0.1279]],\n",
            "\n",
            "        [[-0.0919]],\n",
            "\n",
            "        [[-0.0595]],\n",
            "\n",
            "        [[-0.1167]],\n",
            "\n",
            "        [[-0.0906]],\n",
            "\n",
            "        [[-0.0819]],\n",
            "\n",
            "        [[-0.0866]],\n",
            "\n",
            "        [[-0.0860]],\n",
            "\n",
            "        [[-0.0894]],\n",
            "\n",
            "        [[-0.0940]],\n",
            "\n",
            "        [[-0.1248]],\n",
            "\n",
            "        [[-0.1116]],\n",
            "\n",
            "        [[-0.1273]],\n",
            "\n",
            "        [[-0.1901]],\n",
            "\n",
            "        [[-0.1040]],\n",
            "\n",
            "        [[-0.1103]],\n",
            "\n",
            "        [[-0.0806]],\n",
            "\n",
            "        [[-0.0596]],\n",
            "\n",
            "        [[-0.0899]],\n",
            "\n",
            "        [[-0.0700]],\n",
            "\n",
            "        [[-0.1049]],\n",
            "\n",
            "        [[-0.0851]],\n",
            "\n",
            "        [[-0.1397]],\n",
            "\n",
            "        [[-0.1007]],\n",
            "\n",
            "        [[-0.0799]],\n",
            "\n",
            "        [[-0.1016]],\n",
            "\n",
            "        [[-0.0878]],\n",
            "\n",
            "        [[-0.1159]],\n",
            "\n",
            "        [[-0.0951]],\n",
            "\n",
            "        [[-0.1187]],\n",
            "\n",
            "        [[-0.1249]],\n",
            "\n",
            "        [[-0.0941]],\n",
            "\n",
            "        [[-0.0476]],\n",
            "\n",
            "        [[-0.1704]],\n",
            "\n",
            "        [[-0.0759]],\n",
            "\n",
            "        [[-0.0899]],\n",
            "\n",
            "        [[-0.0733]],\n",
            "\n",
            "        [[-0.1050]],\n",
            "\n",
            "        [[-0.1032]],\n",
            "\n",
            "        [[-0.0746]],\n",
            "\n",
            "        [[-0.1695]],\n",
            "\n",
            "        [[-0.0516]],\n",
            "\n",
            "        [[-0.1044]],\n",
            "\n",
            "        [[-0.0892]],\n",
            "\n",
            "        [[-0.1109]]], grad_fn=<AddBackward0>)\n",
            "answer tensor([[-0.4906],\n",
            "        [ 0.7081],\n",
            "        [-0.8652],\n",
            "        [ 0.5882],\n",
            "        [ 0.2549],\n",
            "        [-0.9026],\n",
            "        [-0.7715],\n",
            "        [ 0.9329],\n",
            "        [-1.5020],\n",
            "        [ 0.6332],\n",
            "        [-0.7902],\n",
            "        [-0.3033],\n",
            "        [ 0.8167],\n",
            "        [-0.1572],\n",
            "        [-0.4063],\n",
            "        [ 1.3262],\n",
            "        [ 0.1425],\n",
            "        [ 1.3824],\n",
            "        [-0.6029],\n",
            "        [-1.2023],\n",
            "        [ 1.0078],\n",
            "        [-0.1534],\n",
            "        [-0.2658],\n",
            "        [ 0.7268],\n",
            "        [-0.4906],\n",
            "        [-0.6779],\n",
            "        [ 0.4459],\n",
            "        [-0.9775],\n",
            "        [-0.0411],\n",
            "        [-0.7528],\n",
            "        [ 2.3188],\n",
            "        [ 0.0751],\n",
            "        [ 0.2961],\n",
            "        [-1.2397],\n",
            "        [ 0.2586],\n",
            "        [-0.5655],\n",
            "        [-0.3595],\n",
            "        [-0.3033],\n",
            "        [-1.0899],\n",
            "        [-0.3033],\n",
            "        [-0.3576],\n",
            "        [ 1.7532],\n",
            "        [ 1.0452],\n",
            "        [-0.9401],\n",
            "        [-0.8277],\n",
            "        [ 0.3710],\n",
            "        [-0.9775],\n",
            "        [-0.5842],\n",
            "        [-0.3033],\n",
            "        [-0.6779],\n",
            "        [ 0.0151],\n",
            "        [ 0.0713],\n",
            "        [ 0.7175],\n",
            "        [-0.6029],\n",
            "        [-0.0036],\n",
            "        [-0.5280],\n",
            "        [ 0.2586],\n",
            "        [-0.4718],\n",
            "        [ 0.0339],\n",
            "        [ 0.2586],\n",
            "        [-0.7153],\n",
            "        [ 0.2586],\n",
            "        [-0.4531],\n",
            "        [ 0.7081],\n",
            "        [-0.9026],\n",
            "        [ 0.3335],\n",
            "        [-1.0524],\n",
            "        [-0.0411],\n",
            "        [-0.0411],\n",
            "        [-0.7153],\n",
            "        [ 0.7456],\n",
            "        [ 2.3188],\n",
            "        [-0.9775],\n",
            "        [ 0.4084],\n",
            "        [ 0.6707],\n",
            "        [-1.1648],\n",
            "        [-0.9401],\n",
            "        [-0.1347],\n",
            "        [-1.1573],\n",
            "        [-0.5468],\n",
            "        [-1.2772],\n",
            "        [-0.3220],\n",
            "        [ 0.6332],\n",
            "        [ 1.8881],\n",
            "        [ 3.5550],\n",
            "        [-1.1274],\n",
            "        [-0.3407],\n",
            "        [-0.3407],\n",
            "        [-1.4270],\n",
            "        [ 0.3710],\n",
            "        [-0.6779],\n",
            "        [ 0.2586],\n",
            "        [ 3.2553],\n",
            "        [ 0.2586],\n",
            "        [ 1.6633],\n",
            "        [-1.0244],\n",
            "        [-0.3969],\n",
            "        [-0.1534],\n",
            "        [-0.6779],\n",
            "        [ 0.2024],\n",
            "        [-0.6029],\n",
            "        [-1.4083],\n",
            "        [ 0.0676],\n",
            "        [-0.4718],\n",
            "        [ 0.1462],\n",
            "        [-0.2621],\n",
            "        [ 1.0078],\n",
            "        [-0.8202],\n",
            "        [ 0.1462]])\n"
          ]
        }
      ]
    }
  ]
}