{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_2_Problem_1.ipynb",
      "provenance": [],
      "mount_file_id": "1PxP7utJcbe9UCG__3rDgNZd05f89W9Yp",
      "authorship_tag": "ABX9TyMCYoooJQWUnrEsP3Qkal9w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrahma15/MyRepos/blob/main/Homework_2_Problem_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BoaX5pNd5Gqt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd  \n",
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "Rc68c8ZhL2dM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing = pd.DataFrame(pd.read_csv(\"/content/drive/MyDrive/Google Colab data/Homework 1/Housing.csv\")) \n",
        "housing.head()"
      ],
      "metadata": {
        "id": "MxFeXklr5ngP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "d475dcf3-b055-45c6-bbba-bfe0931b9678"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-74a72d5d-ac46-4f67-ba03-b826978ef6e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>mainroad</th>\n",
              "      <th>guestroom</th>\n",
              "      <th>basement</th>\n",
              "      <th>hotwaterheating</th>\n",
              "      <th>airconditioning</th>\n",
              "      <th>parking</th>\n",
              "      <th>prefarea</th>\n",
              "      <th>furnishingstatus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13300000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12250000</td>\n",
              "      <td>8960</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12250000</td>\n",
              "      <td>9960</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>semi-furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12215000</td>\n",
              "      <td>7500</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11410000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74a72d5d-ac46-4f67-ba03-b826978ef6e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74a72d5d-ac46-4f67-ba03-b826978ef6e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74a72d5d-ac46-4f67-ba03-b826978ef6e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      price  area  bedrooms  ...  parking  prefarea furnishingstatus\n",
              "0  13300000  7420         4  ...        2       yes        furnished\n",
              "1  12250000  8960         4  ...        3        no        furnished\n",
              "2  12250000  9960         3  ...        2       yes   semi-furnished\n",
              "3  12215000  7500         4  ...        3       yes        furnished\n",
              "4  11410000  7420         4  ...        2        no        furnished\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price'] \n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "housing[num_vars] = scaler.fit_transform(housing[num_vars])\n",
        "\n",
        "Scaled_data = housing[num_vars] \n",
        "Scaled_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9K5HrkzRMMJF",
        "outputId": "c743575c-bab3-4584-9c3d-21dd25cb9dc6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-281936ed-6f62-4376-a6f0-4286d254371e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>parking</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.046726</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>1.378217</td>\n",
              "      <td>1.517692</td>\n",
              "      <td>4.566365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.757010</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>5.405809</td>\n",
              "      <td>2.532024</td>\n",
              "      <td>2.679409</td>\n",
              "      <td>4.004484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.218232</td>\n",
              "      <td>0.047278</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>1.517692</td>\n",
              "      <td>4.004484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.083624</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>2.679409</td>\n",
              "      <td>3.985755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.046726</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>-0.570187</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>1.517692</td>\n",
              "      <td>3.554979</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-281936ed-6f62-4376-a6f0-4286d254371e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-281936ed-6f62-4376-a6f0-4286d254371e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-281936ed-6f62-4376-a6f0-4286d254371e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       area  bedrooms  bathrooms   stories   parking     price\n",
              "0  1.046726  1.403419   1.421812  1.378217  1.517692  4.566365\n",
              "1  1.757010  1.403419   5.405809  2.532024  2.679409  4.004484\n",
              "2  2.218232  0.047278   1.421812  0.224410  1.517692  4.004484\n",
              "3  1.083624  1.403419   1.421812  0.224410  2.679409  3.985755\n",
              "4  1.046726  1.403419  -0.570187  0.224410  1.517692  3.554979"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_Scaled = Scaled_data.pop('price')\n",
        "X_Scaled = Scaled_data"
      ],
      "metadata": {
        "id": "dFp4DyDTQYZ2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Scaled.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q2TSisHFQhBd",
        "outputId": "413041c7-5fed-43c5-cb3b-0df748b9edf1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-138934c5-b2a7-4cd6-b2b1-0d17e7ad54c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>parking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.046726</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>1.378217</td>\n",
              "      <td>1.517692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.757010</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>5.405809</td>\n",
              "      <td>2.532024</td>\n",
              "      <td>2.679409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.218232</td>\n",
              "      <td>0.047278</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>1.517692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.083624</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>1.421812</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>2.679409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.046726</td>\n",
              "      <td>1.403419</td>\n",
              "      <td>-0.570187</td>\n",
              "      <td>0.224410</td>\n",
              "      <td>1.517692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-138934c5-b2a7-4cd6-b2b1-0d17e7ad54c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-138934c5-b2a7-4cd6-b2b1-0d17e7ad54c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-138934c5-b2a7-4cd6-b2b1-0d17e7ad54c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       area  bedrooms  bathrooms   stories   parking\n",
              "0  1.046726  1.403419   1.421812  1.378217  1.517692\n",
              "1  1.757010  1.403419   5.405809  2.532024  2.679409\n",
              "2  2.218232  0.047278   1.421812  0.224410  1.517692\n",
              "3  1.083624  1.403419   1.421812  0.224410  2.679409\n",
              "4  1.046726  1.403419  -0.570187  0.224410  1.517692"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_Scaled.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJd8xgjXQkfp",
        "outputId": "8a713e9e-142c-40dd-bfee-686b78f0caec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4.566365\n",
              "1    4.004484\n",
              "2    4.004484\n",
              "3    3.985755\n",
              "4    3.554979\n",
              "Name: price, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_Scaled = torch.tensor(y_Scaled.values,dtype=torch.float).unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "aDgS5ityQqci"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_Scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q56iFfBSUQ9f",
        "outputId": "3076ee63-6d68-4b67-f2e5-cc8ed36b7556"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.5664e+00],\n",
              "        [ 4.0045e+00],\n",
              "        [ 4.0045e+00],\n",
              "        [ 3.9858e+00],\n",
              "        [ 3.5550e+00],\n",
              "        [ 3.2553e+00],\n",
              "        [ 2.8807e+00],\n",
              "        [ 2.8807e+00],\n",
              "        [ 2.7309e+00],\n",
              "        [ 2.6934e+00],\n",
              "        [ 2.6934e+00],\n",
              "        [ 2.6297e+00],\n",
              "        [ 2.4312e+00],\n",
              "        [ 2.3938e+00],\n",
              "        [ 2.3938e+00],\n",
              "        [ 2.3188e+00],\n",
              "        [ 2.3188e+00],\n",
              "        [ 2.2439e+00],\n",
              "        [ 2.2065e+00],\n",
              "        [ 2.1877e+00],\n",
              "        [ 2.1315e+00],\n",
              "        [ 2.0941e+00],\n",
              "        [ 2.0754e+00],\n",
              "        [ 2.0754e+00],\n",
              "        [ 2.0379e+00],\n",
              "        [ 2.0192e+00],\n",
              "        [ 1.9780e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.9443e+00],\n",
              "        [ 1.8881e+00],\n",
              "        [ 1.8319e+00],\n",
              "        [ 1.7944e+00],\n",
              "        [ 1.7735e+00],\n",
              "        [ 1.7532e+00],\n",
              "        [ 1.7195e+00],\n",
              "        [ 1.7101e+00],\n",
              "        [ 1.6820e+00],\n",
              "        [ 1.6633e+00],\n",
              "        [ 1.6446e+00],\n",
              "        [ 1.5697e+00],\n",
              "        [ 1.5697e+00],\n",
              "        [ 1.4947e+00],\n",
              "        [ 1.4947e+00],\n",
              "        [ 1.4760e+00],\n",
              "        [ 1.4573e+00],\n",
              "        [ 1.4386e+00],\n",
              "        [ 1.4198e+00],\n",
              "        [ 1.4198e+00],\n",
              "        [ 1.4198e+00],\n",
              "        [ 1.3824e+00],\n",
              "        [ 1.3824e+00],\n",
              "        [ 1.3824e+00],\n",
              "        [ 1.3824e+00],\n",
              "        [ 1.3786e+00],\n",
              "        [ 1.3262e+00],\n",
              "        [ 1.3075e+00],\n",
              "        [ 1.3075e+00],\n",
              "        [ 1.2700e+00],\n",
              "        [ 1.2325e+00],\n",
              "        [ 1.2325e+00],\n",
              "        [ 1.2138e+00],\n",
              "        [ 1.1951e+00],\n",
              "        [ 1.1576e+00],\n",
              "        [ 1.1576e+00],\n",
              "        [ 1.1389e+00],\n",
              "        [ 1.1202e+00],\n",
              "        [ 1.0827e+00],\n",
              "        [ 1.0827e+00],\n",
              "        [ 1.0640e+00],\n",
              "        [ 1.0452e+00],\n",
              "        [ 1.0265e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 1.0078e+00],\n",
              "        [ 9.9655e-01],\n",
              "        [ 9.8906e-01],\n",
              "        [ 9.8906e-01],\n",
              "        [ 9.7033e-01],\n",
              "        [ 9.3287e-01],\n",
              "        [ 9.3287e-01],\n",
              "        [ 9.3287e-01],\n",
              "        [ 9.1414e-01],\n",
              "        [ 9.1414e-01],\n",
              "        [ 8.9541e-01],\n",
              "        [ 8.9541e-01],\n",
              "        [ 8.8417e-01],\n",
              "        [ 8.7668e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.2049e-01],\n",
              "        [ 8.1675e-01],\n",
              "        [ 8.0176e-01],\n",
              "        [ 7.8303e-01],\n",
              "        [ 7.8303e-01],\n",
              "        [ 7.6430e-01],\n",
              "        [ 7.6430e-01],\n",
              "        [ 7.6430e-01],\n",
              "        [ 7.4557e-01],\n",
              "        [ 7.4557e-01],\n",
              "        [ 7.2684e-01],\n",
              "        [ 7.1748e-01],\n",
              "        [ 7.0812e-01],\n",
              "        [ 7.0812e-01],\n",
              "        [ 7.0812e-01],\n",
              "        [ 7.0437e-01],\n",
              "        [ 7.0437e-01],\n",
              "        [ 6.7066e-01],\n",
              "        [ 6.7066e-01],\n",
              "        [ 6.7066e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.3320e-01],\n",
              "        [ 6.2945e-01],\n",
              "        [ 5.9574e-01],\n",
              "        [ 5.9574e-01],\n",
              "        [ 5.9199e-01],\n",
              "        [ 5.9199e-01],\n",
              "        [ 5.8825e-01],\n",
              "        [ 5.5828e-01],\n",
              "        [ 5.5828e-01],\n",
              "        [ 5.5828e-01],\n",
              "        [ 5.5453e-01],\n",
              "        [ 5.3955e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 5.2082e-01],\n",
              "        [ 4.7400e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.4590e-01],\n",
              "        [ 4.2717e-01],\n",
              "        [ 4.2717e-01],\n",
              "        [ 4.0845e-01],\n",
              "        [ 4.0845e-01],\n",
              "        [ 4.0845e-01],\n",
              "        [ 4.0470e-01],\n",
              "        [ 3.8972e-01],\n",
              "        [ 3.8972e-01],\n",
              "        [ 3.7099e-01],\n",
              "        [ 3.7099e-01],\n",
              "        [ 3.7099e-01],\n",
              "        [ 3.7099e-01],\n",
              "        [ 3.5226e-01],\n",
              "        [ 3.3353e-01],\n",
              "        [ 3.2978e-01],\n",
              "        [ 2.9607e-01],\n",
              "        [ 2.7734e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5861e-01],\n",
              "        [ 2.5486e-01],\n",
              "        [ 2.4737e-01],\n",
              "        [ 2.3988e-01],\n",
              "        [ 2.3988e-01],\n",
              "        [ 2.3988e-01],\n",
              "        [ 2.0242e-01],\n",
              "        [ 2.0242e-01],\n",
              "        [ 1.8369e-01],\n",
              "        [ 1.8369e-01],\n",
              "        [ 1.8369e-01],\n",
              "        [ 1.8369e-01],\n",
              "        [ 1.6496e-01],\n",
              "        [ 1.4623e-01],\n",
              "        [ 1.4623e-01],\n",
              "        [ 1.4623e-01],\n",
              "        [ 1.4623e-01],\n",
              "        [ 1.4249e-01],\n",
              "        [ 1.2750e-01],\n",
              "        [ 1.0878e-01],\n",
              "        [ 1.0878e-01],\n",
              "        [ 1.0128e-01],\n",
              "        [ 9.0046e-02],\n",
              "        [ 7.5062e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 7.1316e-02],\n",
              "        [ 6.7571e-02],\n",
              "        [ 6.7571e-02],\n",
              "        [ 5.2587e-02],\n",
              "        [ 3.3858e-02],\n",
              "        [ 3.3858e-02],\n",
              "        [ 3.3858e-02],\n",
              "        [ 3.3858e-02],\n",
              "        [ 1.5128e-02],\n",
              "        [ 1.5128e-02],\n",
              "        [ 1.4489e-04],\n",
              "        [-3.6010e-03],\n",
              "        [-3.6010e-03],\n",
              "        [-3.6010e-03],\n",
              "        [-7.3469e-03],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-4.1060e-02],\n",
              "        [-5.9789e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-7.8518e-02],\n",
              "        [-8.2264e-02],\n",
              "        [-9.7248e-02],\n",
              "        [-9.7248e-02],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1598e-01],\n",
              "        [-1.1972e-01],\n",
              "        [-1.1972e-01],\n",
              "        [-1.3471e-01],\n",
              "        [-1.3471e-01],\n",
              "        [-1.3471e-01],\n",
              "        [-1.3471e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5344e-01],\n",
              "        [-1.5718e-01],\n",
              "        [-1.5718e-01],\n",
              "        [-1.5718e-01],\n",
              "        [-1.7217e-01],\n",
              "        [-1.9089e-01],\n",
              "        [-1.9089e-01],\n",
              "        [-1.9464e-01],\n",
              "        [-1.9464e-01],\n",
              "        [-1.9464e-01],\n",
              "        [-2.0588e-01],\n",
              "        [-2.0962e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.2835e-01],\n",
              "        [-2.3959e-01],\n",
              "        [-2.4708e-01],\n",
              "        [-2.4708e-01],\n",
              "        [-2.6207e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.6581e-01],\n",
              "        [-2.8454e-01],\n",
              "        [-2.8454e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0327e-01],\n",
              "        [-3.0702e-01],\n",
              "        [-3.0702e-01],\n",
              "        [-3.2200e-01],\n",
              "        [-3.2200e-01],\n",
              "        [-3.2200e-01],\n",
              "        [-3.4073e-01],\n",
              "        [-3.4073e-01],\n",
              "        [-3.4448e-01],\n",
              "        [-3.5759e-01],\n",
              "        [-3.5946e-01],\n",
              "        [-3.5946e-01],\n",
              "        [-3.5946e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.7819e-01],\n",
              "        [-3.9692e-01],\n",
              "        [-3.9692e-01],\n",
              "        [-3.9692e-01],\n",
              "        [-4.0628e-01],\n",
              "        [-4.0628e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.1565e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.5311e-01],\n",
              "        [-4.7184e-01],\n",
              "        [-4.7184e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9056e-01],\n",
              "        [-4.9806e-01],\n",
              "        [-5.0929e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.2802e-01],\n",
              "        [-5.3177e-01],\n",
              "        [-5.3177e-01],\n",
              "        [-5.3177e-01],\n",
              "        [-5.4675e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6548e-01],\n",
              "        [-5.6923e-01],\n",
              "        [-5.6923e-01],\n",
              "        [-5.8421e-01],\n",
              "        [-5.8421e-01],\n",
              "        [-5.8421e-01],\n",
              "        [-5.8421e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0294e-01],\n",
              "        [-6.0669e-01],\n",
              "        [-6.2167e-01],\n",
              "        [-6.2167e-01],\n",
              "        [-6.4040e-01],\n",
              "        [-6.4040e-01],\n",
              "        [-6.4040e-01],\n",
              "        [-6.4040e-01],\n",
              "        [-6.5913e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.7786e-01],\n",
              "        [-6.8160e-01],\n",
              "        [-6.9659e-01],\n",
              "        [-6.9659e-01],\n",
              "        [-6.9659e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1532e-01],\n",
              "        [-7.1906e-01],\n",
              "        [-7.3405e-01],\n",
              "        [-7.3405e-01],\n",
              "        [-7.3405e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5278e-01],\n",
              "        [-7.5652e-01],\n",
              "        [-7.6776e-01],\n",
              "        [-7.7151e-01],\n",
              "        [-7.7151e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-7.9023e-01],\n",
              "        [-8.0896e-01],\n",
              "        [-8.0896e-01],\n",
              "        [-8.2020e-01],\n",
              "        [-8.2769e-01],\n",
              "        [-8.2769e-01],\n",
              "        [-8.2769e-01],\n",
              "        [-8.2769e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6515e-01],\n",
              "        [-8.6890e-01],\n",
              "        [-8.7639e-01],\n",
              "        [-8.8182e-01],\n",
              "        [-8.8388e-01],\n",
              "        [-8.8388e-01],\n",
              "        [-8.8388e-01],\n",
              "        [-8.9886e-01],\n",
              "        [-9.0261e-01],\n",
              "        [-9.0261e-01],\n",
              "        [-9.0261e-01],\n",
              "        [-9.0261e-01],\n",
              "        [-9.2134e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4007e-01],\n",
              "        [-9.4382e-01],\n",
              "        [-9.5880e-01],\n",
              "        [-9.6629e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-9.7753e-01],\n",
              "        [-1.0150e+00],\n",
              "        [-1.0150e+00],\n",
              "        [-1.0150e+00],\n",
              "        [-1.0150e+00],\n",
              "        [-1.0244e+00],\n",
              "        [-1.0337e+00],\n",
              "        [-1.0337e+00],\n",
              "        [-1.0337e+00],\n",
              "        [-1.0524e+00],\n",
              "        [-1.0524e+00],\n",
              "        [-1.0899e+00],\n",
              "        [-1.0899e+00],\n",
              "        [-1.1086e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1274e+00],\n",
              "        [-1.1311e+00],\n",
              "        [-1.1311e+00],\n",
              "        [-1.1573e+00],\n",
              "        [-1.1648e+00],\n",
              "        [-1.1648e+00],\n",
              "        [-1.1648e+00],\n",
              "        [-1.2023e+00],\n",
              "        [-1.2023e+00],\n",
              "        [-1.2023e+00],\n",
              "        [-1.2210e+00],\n",
              "        [-1.2210e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2397e+00],\n",
              "        [-1.2622e+00],\n",
              "        [-1.2772e+00],\n",
              "        [-1.2772e+00],\n",
              "        [-1.2772e+00],\n",
              "        [-1.2959e+00],\n",
              "        [-1.3147e+00],\n",
              "        [-1.3334e+00],\n",
              "        [-1.3334e+00],\n",
              "        [-1.3334e+00],\n",
              "        [-1.3521e+00],\n",
              "        [-1.3559e+00],\n",
              "        [-1.4083e+00],\n",
              "        [-1.4270e+00],\n",
              "        [-1.4270e+00],\n",
              "        [-1.4270e+00],\n",
              "        [-1.5020e+00],\n",
              "        [-1.5394e+00],\n",
              "        [-1.5394e+00],\n",
              "        [-1.5581e+00],\n",
              "        [-1.5769e+00],\n",
              "        [-1.6051e+00],\n",
              "        [-1.6143e+00],\n",
              "        [-1.6143e+00],\n",
              "        [-1.6143e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Scaled = torch.tensor(X_Scaled.values,dtype=torch.float).unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "D4WmxgOoTHHJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1ak0XjDUItm",
        "outputId": "c570d429-ea1a-485c-a9e5-9295b5ec2d4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.0467,  1.4034,  ...,  1.3782,  1.5177]],\n",
              "\n",
              "        [[ 1.7570,  1.4034,  ...,  2.5320,  2.6794]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-1.0334,  0.0473,  ..., -0.9294, -0.8057]],\n",
              "\n",
              "        [[-0.5998,  0.0473,  ...,  0.2244, -0.8057]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = X_Scaled.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_indices, val_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_yQhfXGVApI",
        "outputId": "091012b3-3008-4215-c4a7-9dbec5f05c34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([147,  36, 521, 304,  70,  87, 238, 171, 395, 407, 444,  54, 162,\n",
              "         231, 198,  75, 152, 362, 414, 226, 451, 195, 100,   6, 435, 363,\n",
              "         317, 182, 443, 396, 320, 271, 538, 300, 138,  60, 249, 316, 383,\n",
              "         227, 259,  50, 223, 525,  98, 402, 289, 422, 434,  17,  10, 436,\n",
              "         156,  44, 471, 269,  52, 517,  16, 281, 145,  64,  14,  79, 248,\n",
              "          65, 181,   0, 394, 117, 369, 186, 461, 450,  86,  23,  11,  92,\n",
              "         518, 107, 424, 233,  53, 385,   2, 282, 124, 243, 146, 151, 514,\n",
              "         311, 345, 397, 367, 341, 532, 265, 361, 105, 157, 343, 506, 280,\n",
              "         193, 329, 505, 273,  33, 203, 128, 427, 382, 246, 185, 481, 287,\n",
              "         490, 326, 276,  13, 371, 260,  77,  35, 219, 500, 358, 499,  91,\n",
              "         429, 489, 174, 493, 241, 468, 352, 194,  37, 188, 356, 488, 420,\n",
              "          72, 507, 211, 155, 389, 366, 189, 115, 126, 131, 264,  28,   9,\n",
              "         486, 103, 520, 274, 200, 144,  90, 355, 426, 168,  82, 332, 206,\n",
              "         412, 177,  56, 175, 342, 365, 251, 360, 515, 498, 154, 452, 161,\n",
              "         253,  96, 527, 190, 217, 285,  55, 101, 278, 257,  47, 413, 163,\n",
              "         502, 202, 130, 140, 197, 299, 306, 167, 350,  69, 391, 359,   4,\n",
              "         245, 354, 428, 454, 192, 172, 501, 213, 472,   1,  26,  24, 377,\n",
              "         261, 535, 322, 441, 277, 215, 102,  76, 340,   5, 240, 160, 529,\n",
              "         298, 536, 478, 323, 166, 534, 178, 176,  74, 411, 415, 158, 284,\n",
              "         153, 142, 421, 252,  66, 266, 112, 475, 344, 333, 324,   8, 165,\n",
              "         388, 331, 448, 432, 180, 111, 542, 406, 104, 187, 544, 401, 403,\n",
              "         286,  67, 283,  68, 106, 349,  59, 370, 335, 239, 399, 368, 522,\n",
              "         129,  22, 308,  84, 318, 307,  73, 113, 480,  41, 409, 348, 473,\n",
              "         469, 519, 528, 263, 234, 477, 204, 123, 456, 301, 120,  97, 433,\n",
              "         508, 296, 235, 267,  21, 221, 531, 453, 465, 338, 207,  25, 325,\n",
              "          18, 255, 141,   7, 376, 315,  12,  89, 169, 449, 425, 220,  45,\n",
              "         374, 150, 440, 116,  38, 295, 466, 309, 523, 268, 279, 384, 183,\n",
              "         110, 511,  30,  83, 487, 400, 119, 417,  31, 139, 398,  20,  32,\n",
              "         393, 210, 288, 336, 442, 423,  42, 314,  63, 539, 143, 173, 114,\n",
              "         446, 339, 353, 118, 437, 302,  40, 479, 459,  81, 109,  85, 494,\n",
              "          88, 334, 476, 214, 208, 464,  58, 346, 133, 237, 364, 405, 303,\n",
              "         148, 170, 386, 381, 228,  19, 387, 218,  61,  57, 463, 510, 224,\n",
              "         212,  95, 419, 497, 191, 305, 447, 496,  39,  71, 483, 294, 330,\n",
              "         122,  94, 134, 222, 430,  78,  62]),\n",
              " tensor([524, 416, 125, 230, 372, 540, 179, 445, 457, 290, 503, 216,  15,\n",
              "         351,  43, 321, 319, 431, 390, 272, 378,  48, 137, 408, 439, 196,\n",
              "         242, 438, 458, 262, 108, 310, 199, 225,  51, 136, 201, 455, 270,\n",
              "         467, 313, 495, 236, 357, 533,  27, 232, 410,  80, 530, 375, 482,\n",
              "         149, 244, 404, 184,  99, 256, 250, 460, 254, 292, 516, 526, 293,\n",
              "         470, 258, 275, 132, 291, 337, 135, 379,  93, 392, 159, 418, 513,\n",
              "         312,  49, 491, 512, 462, 380, 209, 543,   3, 328, 229,  34, 541,\n",
              "         484, 485, 509, 121, 164, 205,  46, 297, 492, 504, 327, 127, 474,\n",
              "          29, 537, 247, 373, 347]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = X_Scaled[train_indices]\n",
        "y_Train = y_Scaled[train_indices]\n",
        "\n",
        "X_val = X_Scaled[val_indices]\n",
        "y_val = y_Scaled[val_indices]"
      ],
      "metadata": {
        "id": "V5OVIEG3Viez"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "linear_model = nn.Linear(5, 1) # <1>\n",
        "linear_model(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzIFjLWYWEVc",
        "outputId": "2d7830e7-2eab-40a6-dc96-7d1821676473"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.4894]],\n",
              "\n",
              "        [[ 0.5062]],\n",
              "\n",
              "        [[-1.6001]],\n",
              "\n",
              "        [[ 0.9868]],\n",
              "\n",
              "        [[ 0.7739]],\n",
              "\n",
              "        [[ 1.1820]],\n",
              "\n",
              "        [[ 0.4192]],\n",
              "\n",
              "        [[ 2.0549]],\n",
              "\n",
              "        [[ 0.9373]],\n",
              "\n",
              "        [[ 0.2718]],\n",
              "\n",
              "        [[ 0.7640]],\n",
              "\n",
              "        [[ 0.0572]],\n",
              "\n",
              "        [[-0.6659]],\n",
              "\n",
              "        [[ 1.4311]],\n",
              "\n",
              "        [[-0.9320]],\n",
              "\n",
              "        [[ 0.3412]],\n",
              "\n",
              "        [[-0.2781]],\n",
              "\n",
              "        [[ 0.1761]],\n",
              "\n",
              "        [[ 0.9535]],\n",
              "\n",
              "        [[ 0.3979]],\n",
              "\n",
              "        [[ 0.7985]],\n",
              "\n",
              "        [[ 0.4016]],\n",
              "\n",
              "        [[-0.2536]],\n",
              "\n",
              "        [[ 1.3618]],\n",
              "\n",
              "        [[ 1.3739]],\n",
              "\n",
              "        [[ 0.1250]],\n",
              "\n",
              "        [[ 0.6941]],\n",
              "\n",
              "        [[ 1.2751]],\n",
              "\n",
              "        [[ 0.6577]],\n",
              "\n",
              "        [[ 1.4051]],\n",
              "\n",
              "        [[-0.1432]],\n",
              "\n",
              "        [[ 0.4199]],\n",
              "\n",
              "        [[ 0.4205]],\n",
              "\n",
              "        [[ 0.4091]],\n",
              "\n",
              "        [[-0.2125]],\n",
              "\n",
              "        [[-0.5635]],\n",
              "\n",
              "        [[ 0.6152]],\n",
              "\n",
              "        [[ 0.5048]],\n",
              "\n",
              "        [[ 0.2346]],\n",
              "\n",
              "        [[ 0.4103]],\n",
              "\n",
              "        [[ 0.6856]],\n",
              "\n",
              "        [[ 0.4551]],\n",
              "\n",
              "        [[ 1.4254]],\n",
              "\n",
              "        [[-0.6506]],\n",
              "\n",
              "        [[ 0.9091]],\n",
              "\n",
              "        [[-0.2577]],\n",
              "\n",
              "        [[ 0.6759]],\n",
              "\n",
              "        [[ 0.7900]],\n",
              "\n",
              "        [[ 0.1084]],\n",
              "\n",
              "        [[ 1.1194]],\n",
              "\n",
              "        [[ 0.2919]],\n",
              "\n",
              "        [[ 0.7791]],\n",
              "\n",
              "        [[-0.2861]],\n",
              "\n",
              "        [[ 0.4029]],\n",
              "\n",
              "        [[ 0.4757]],\n",
              "\n",
              "        [[ 0.8035]],\n",
              "\n",
              "        [[-0.4451]],\n",
              "\n",
              "        [[ 0.2786]],\n",
              "\n",
              "        [[ 0.5844]],\n",
              "\n",
              "        [[ 0.4745]],\n",
              "\n",
              "        [[-0.4076]],\n",
              "\n",
              "        [[ 0.2496]],\n",
              "\n",
              "        [[ 1.3170]],\n",
              "\n",
              "        [[ 1.5039]],\n",
              "\n",
              "        [[ 1.1142]],\n",
              "\n",
              "        [[ 0.5775]],\n",
              "\n",
              "        [[ 0.4482]],\n",
              "\n",
              "        [[ 1.3562]],\n",
              "\n",
              "        [[ 0.2914]],\n",
              "\n",
              "        [[ 0.8132]],\n",
              "\n",
              "        [[ 0.4862]],\n",
              "\n",
              "        [[ 0.0189]],\n",
              "\n",
              "        [[ 0.4941]],\n",
              "\n",
              "        [[-0.3220]],\n",
              "\n",
              "        [[ 0.6334]],\n",
              "\n",
              "        [[ 0.9098]],\n",
              "\n",
              "        [[-0.2325]],\n",
              "\n",
              "        [[ 0.5624]],\n",
              "\n",
              "        [[ 0.8333]],\n",
              "\n",
              "        [[ 0.1661]],\n",
              "\n",
              "        [[ 1.4210]],\n",
              "\n",
              "        [[ 1.4029]],\n",
              "\n",
              "        [[ 0.9507]],\n",
              "\n",
              "        [[ 1.2751]],\n",
              "\n",
              "        [[ 0.2925]],\n",
              "\n",
              "        [[ 0.9529]],\n",
              "\n",
              "        [[-1.1041]],\n",
              "\n",
              "        [[ 0.5435]],\n",
              "\n",
              "        [[-1.1266]],\n",
              "\n",
              "        [[-1.2328]],\n",
              "\n",
              "        [[ 1.0414]],\n",
              "\n",
              "        [[ 1.5282]],\n",
              "\n",
              "        [[ 1.4259]],\n",
              "\n",
              "        [[ 1.1207]],\n",
              "\n",
              "        [[ 0.0716]],\n",
              "\n",
              "        [[-0.0162]],\n",
              "\n",
              "        [[ 0.0121]],\n",
              "\n",
              "        [[-0.1577]],\n",
              "\n",
              "        [[ 0.6926]],\n",
              "\n",
              "        [[ 0.6892]],\n",
              "\n",
              "        [[ 1.5031]],\n",
              "\n",
              "        [[ 0.0252]],\n",
              "\n",
              "        [[ 0.0645]],\n",
              "\n",
              "        [[-0.2037]],\n",
              "\n",
              "        [[-0.4042]],\n",
              "\n",
              "        [[ 1.0304]],\n",
              "\n",
              "        [[-1.5231]],\n",
              "\n",
              "        [[ 1.4029]],\n",
              "\n",
              "        [[ 0.7444]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtOavAuwXvp6",
        "outputId": "cbcdec84-047b-4056-b798-7c96dc6aafff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.3759, -0.4408, -0.0008, -0.1147, -0.1520]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Kz-dsRXwOT",
        "outputId": "971d8af5-1d03-4f05-9c83-e38554b0f892"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0.3559], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(5)\n",
        "linear_model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3MNPTx9X291",
        "outputId": "12093353-13ef-43ba-c332-f3a842851341"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.7282], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(10, 5)\n",
        "linear_model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDLfG6fRX-M8",
        "outputId": "1e2fcefb-35a2-49df-c156-827913fb87c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7282],\n",
              "        [-0.7282],\n",
              "        [-0.7282],\n",
              "        [-0.7282],\n",
              "        [-0.7282],\n",
              "        [-0.7282],\n",
              "        [-0.7282],\n",
              "        [-0.7282],\n",
              "        [-0.7282],\n",
              "        [-0.7282]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = nn.Linear(5, 1) # <1>\n",
        "optimizer = optim.SGD(\n",
        "    linear_model.parameters(), # <2>\n",
        "    lr=1e-2)"
      ],
      "metadata": {
        "id": "Ifydd0dCYEAS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCSh-Ifedqjo",
        "outputId": "5610d015-385e-4da8-eb79-1a07b9c61091"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7fb9ebddbdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(linear_model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSoqnFh2drad",
        "outputId": "5775b9ad-b541-4735-ff22-565306c87763"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.2836,  0.1755, -0.3297, -0.2450, -0.4403]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([0.0489], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, X_Train, X_val,\n",
        "                  y_Train, y_val):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        y_p_Train = model(X_Train) # <1>\n",
        "        loss_train = loss_fn(y_p_Train, y_Train)\n",
        "\n",
        "        t_p_val = model(X_val) # <1>\n",
        "        loss_val = loss_fn(y_p_Train, y_val)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward() # <2>\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch == 1 or epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
        "                  f\" Validation loss {loss_val.item():.4f}\")"
      ],
      "metadata": {
        "id": "qePIqhukduWM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_p_Train, y_Train):\n",
        "    squared_diffs = (y_p_Train - y_Train)**2\n",
        "    return squared_diffs.mean()\n",
        "\n",
        "linear_model = nn.Linear(5, 1) # <1>\n",
        "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = linear_model,\n",
        "    loss_fn = loss_fn,\n",
        "    X_Train = X_Train,\n",
        "    X_val = X_val, \n",
        "    y_Train = y_Train,\n",
        "    y_val = y_val)\n",
        "\n",
        "print()\n",
        "print(linear_model.weight)\n",
        "print(linear_model.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJafXhs1d1R_",
        "outputId": "c90270d0-40db-4757-d512-944f60418959"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.4981, Validation loss 1.5341\n",
            "Epoch 20, Training loss 1.2135, Validation loss 1.2046\n",
            "Epoch 40, Training loss 1.0972, Validation loss 1.0580\n",
            "Epoch 60, Training loss 1.0524, Validation loss 0.9940\n",
            "Epoch 80, Training loss 1.0347, Validation loss 0.9641\n",
            "Epoch 100, Training loss 1.0274, Validation loss 0.9490\n",
            "Epoch 120, Training loss 1.0243, Validation loss 0.9410\n",
            "Epoch 140, Training loss 1.0229, Validation loss 0.9364\n",
            "Epoch 160, Training loss 1.0222, Validation loss 0.9336\n",
            "Epoch 180, Training loss 1.0218, Validation loss 0.9320\n",
            "Epoch 200, Training loss 1.0217, Validation loss 0.9310\n",
            "\n",
            "Parameter containing:\n",
            "tensor([[ 0.0034,  0.0060,  0.0078, -0.0153, -0.0045]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0570], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = nn.Linear(5, 1)\n",
        "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = linear_model,\n",
        "    loss_fn = nn.MSELoss(), # <1>\n",
        "    X_Train = X_Train,\n",
        "    X_val = X_val, \n",
        "    y_Train = y_Train,\n",
        "    y_val = y_val)\n",
        "\n",
        "print()\n",
        "print(linear_model.weight)\n",
        "print(linear_model.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3PHWXXtd53h",
        "outputId": "f7129d68-ea58-4c48-f6a8-86910d06f178"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([436, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([109, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.3485, Validation loss 1.1479\n",
            "Epoch 20, Training loss 1.1626, Validation loss 0.9927\n",
            "Epoch 40, Training loss 1.0821, Validation loss 0.9357\n",
            "Epoch 60, Training loss 1.0487, Validation loss 0.9189\n",
            "Epoch 80, Training loss 1.0343, Validation loss 0.9161\n",
            "Epoch 100, Training loss 1.0278, Validation loss 0.9177\n",
            "Epoch 120, Training loss 1.0247, Validation loss 0.9202\n",
            "Epoch 140, Training loss 1.0232, Validation loss 0.9226\n",
            "Epoch 160, Training loss 1.0224, Validation loss 0.9244\n",
            "Epoch 180, Training loss 1.0220, Validation loss 0.9258\n",
            "Epoch 200, Training loss 1.0217, Validation loss 0.9268\n",
            "\n",
            "Parameter containing:\n",
            "tensor([[-0.0011, -0.0144, -0.0019,  0.0135,  0.0090]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0494], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model = nn.Sequential(\n",
        "            nn.Linear(5, 8), # <1>\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(8, 1)) # <2>\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoj6arV9d_Ze",
        "outputId": "3c518891-f261-4dac-becf-adac8e4daab8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[param.shape for param in seq_model.parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdMa4q40eESk",
        "outputId": "67975a80-fb9d-490b-9dd0-1bbfc79c0869"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([8, 5]), torch.Size([8]), torch.Size([1, 8]), torch.Size([1])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WphCE783eIUc",
        "outputId": "c29798ef-02c0-4e1d-f04a-801f084c907e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.weight torch.Size([8, 5])\n",
            "0.bias torch.Size([8])\n",
            "2.weight torch.Size([1, 8])\n",
            "2.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "seq_model = nn.Sequential(OrderedDict([\n",
        "    ('hidden_linear', nn.Linear(5, 8)),\n",
        "    ('hidden_activation', nn.Tanh()),\n",
        "    ('output_linear', nn.Linear(8, 1))\n",
        "]))\n",
        "\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihueAeyteUBh",
        "outputId": "4352fa78-1d56-4a65-d0b8-d4c9a2728881"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6I7nDgBeW1R",
        "outputId": "bbf9f2bb-7c7d-487a-e417-648664951ae1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_linear.weight torch.Size([8, 5])\n",
            "hidden_linear.bias torch.Size([8])\n",
            "output_linear.weight torch.Size([1, 8])\n",
            "output_linear.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model.output_linear.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5npBO0uJebTO",
        "outputId": "bb645157-bdb6-4623-d86b-fe1828190b54"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.1921], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3) # <1>\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    X_Train = X_Train,\n",
        "    X_val = X_val, \n",
        "    y_Train = y_Train,\n",
        "    y_val = y_val)\n",
        "    \n",
        "print('output', seq_model(X_val))\n",
        "print('answer', y_val)\n",
        "print('hidden', seq_model.hidden_linear.weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFu6fNDRegDH",
        "outputId": "ac7acd0b-9e9d-46ec-9de1-d741b824ca1b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.0814, Validation loss 0.9329\n",
            "Epoch 20, Training loss 1.0780, Validation loss 0.9323\n",
            "Epoch 40, Training loss 1.0746, Validation loss 0.9318\n",
            "Epoch 60, Training loss 1.0715, Validation loss 0.9313\n",
            "Epoch 80, Training loss 1.0686, Validation loss 0.9309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([436, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([109, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Training loss 1.0660, Validation loss 0.9306\n",
            "Epoch 120, Training loss 1.0635, Validation loss 0.9303\n",
            "Epoch 140, Training loss 1.0612, Validation loss 0.9301\n",
            "Epoch 160, Training loss 1.0590, Validation loss 0.9299\n",
            "Epoch 180, Training loss 1.0570, Validation loss 0.9297\n",
            "Epoch 200, Training loss 1.0551, Validation loss 0.9296\n",
            "output tensor([[[-2.5791e-01]],\n",
            "\n",
            "        [[-1.4784e-01]],\n",
            "\n",
            "        [[ 1.8362e-01]],\n",
            "\n",
            "        [[ 3.3998e-03]],\n",
            "\n",
            "        [[-8.3127e-02]],\n",
            "\n",
            "        [[-1.1591e-01]],\n",
            "\n",
            "        [[ 6.5978e-02]],\n",
            "\n",
            "        [[-2.2669e-01]],\n",
            "\n",
            "        [[-2.4383e-01]],\n",
            "\n",
            "        [[ 7.8196e-02]],\n",
            "\n",
            "        [[-2.1870e-01]],\n",
            "\n",
            "        [[-8.0256e-02]],\n",
            "\n",
            "        [[-5.0012e-02]],\n",
            "\n",
            "        [[-2.4898e-01]],\n",
            "\n",
            "        [[ 3.5721e-01]],\n",
            "\n",
            "        [[ 8.3120e-02]],\n",
            "\n",
            "        [[ 4.0530e-03]],\n",
            "\n",
            "        [[-1.2942e-01]],\n",
            "\n",
            "        [[-2.4665e-02]],\n",
            "\n",
            "        [[-1.3097e-01]],\n",
            "\n",
            "        [[ 3.2843e-02]],\n",
            "\n",
            "        [[ 1.0685e-01]],\n",
            "\n",
            "        [[-8.4290e-02]],\n",
            "\n",
            "        [[-2.3724e-01]],\n",
            "\n",
            "        [[-2.3939e-01]],\n",
            "\n",
            "        [[-9.1879e-02]],\n",
            "\n",
            "        [[-6.3259e-02]],\n",
            "\n",
            "        [[-2.2082e-01]],\n",
            "\n",
            "        [[-5.4270e-02]],\n",
            "\n",
            "        [[-2.4472e-01]],\n",
            "\n",
            "        [[ 1.9169e-02]],\n",
            "\n",
            "        [[-1.5787e-01]],\n",
            "\n",
            "        [[ 7.2902e-03]],\n",
            "\n",
            "        [[ 1.9606e-02]],\n",
            "\n",
            "        [[ 3.2818e-01]],\n",
            "\n",
            "        [[ 4.4471e-02]],\n",
            "\n",
            "        [[-4.3869e-02]],\n",
            "\n",
            "        [[-1.7447e-01]],\n",
            "\n",
            "        [[ 2.2680e-01]],\n",
            "\n",
            "        [[-2.7224e-02]],\n",
            "\n",
            "        [[ 2.5303e-02]],\n",
            "\n",
            "        [[-1.5817e-03]],\n",
            "\n",
            "        [[-8.7384e-02]],\n",
            "\n",
            "        [[-9.7784e-03]],\n",
            "\n",
            "        [[-1.1702e-01]],\n",
            "\n",
            "        [[-7.1929e-03]],\n",
            "\n",
            "        [[-5.8757e-02]],\n",
            "\n",
            "        [[-2.2267e-01]],\n",
            "\n",
            "        [[ 8.1856e-02]],\n",
            "\n",
            "        [[-2.6744e-01]],\n",
            "\n",
            "        [[ 3.0722e-02]],\n",
            "\n",
            "        [[-8.4429e-02]],\n",
            "\n",
            "        [[-4.0280e-02]],\n",
            "\n",
            "        [[ 6.1807e-03]],\n",
            "\n",
            "        [[-1.0558e-02]],\n",
            "\n",
            "        [[ 2.6978e-03]],\n",
            "\n",
            "        [[ 1.3710e-01]],\n",
            "\n",
            "        [[ 5.6338e-03]],\n",
            "\n",
            "        [[ 9.5477e-02]],\n",
            "\n",
            "        [[ 2.6999e-02]],\n",
            "\n",
            "        [[-9.7048e-02]],\n",
            "\n",
            "        [[-7.5610e-03]],\n",
            "\n",
            "        [[-1.6205e-01]],\n",
            "\n",
            "        [[-2.6002e-01]],\n",
            "\n",
            "        [[-1.2618e-01]],\n",
            "\n",
            "        [[-1.8780e-01]],\n",
            "\n",
            "        [[ 1.9906e-04]],\n",
            "\n",
            "        [[-2.3625e-01]],\n",
            "\n",
            "        [[ 1.6383e-01]],\n",
            "\n",
            "        [[-9.2987e-02]],\n",
            "\n",
            "        [[-1.1884e-01]],\n",
            "\n",
            "        [[ 3.1978e-01]],\n",
            "\n",
            "        [[-1.4594e-01]],\n",
            "\n",
            "        [[ 5.0999e-02]],\n",
            "\n",
            "        [[-4.8314e-02]],\n",
            "\n",
            "        [[-1.0180e-01]],\n",
            "\n",
            "        [[-1.2630e-01]],\n",
            "\n",
            "        [[-3.1077e-02]],\n",
            "\n",
            "        [[-2.2912e-01]],\n",
            "\n",
            "        [[-1.1052e-03]],\n",
            "\n",
            "        [[-1.7832e-01]],\n",
            "\n",
            "        [[-8.2801e-02]],\n",
            "\n",
            "        [[-1.2740e-01]],\n",
            "\n",
            "        [[-2.2082e-01]],\n",
            "\n",
            "        [[-1.3090e-01]],\n",
            "\n",
            "        [[-2.4596e-01]],\n",
            "\n",
            "        [[ 8.2658e-02]],\n",
            "\n",
            "        [[ 5.3510e-02]],\n",
            "\n",
            "        [[ 1.8850e-01]],\n",
            "\n",
            "        [[-1.5316e-01]],\n",
            "\n",
            "        [[-2.5762e-01]],\n",
            "\n",
            "        [[-2.6342e-01]],\n",
            "\n",
            "        [[-2.4814e-01]],\n",
            "\n",
            "        [[ 4.7869e-02]],\n",
            "\n",
            "        [[ 7.5880e-02]],\n",
            "\n",
            "        [[ 1.9618e-01]],\n",
            "\n",
            "        [[-7.3790e-02]],\n",
            "\n",
            "        [[ 3.6565e-01]],\n",
            "\n",
            "        [[ 2.3950e-02]],\n",
            "\n",
            "        [[-6.2777e-02]],\n",
            "\n",
            "        [[-2.5990e-01]],\n",
            "\n",
            "        [[ 9.9584e-02]],\n",
            "\n",
            "        [[ 2.5914e-01]],\n",
            "\n",
            "        [[-9.3654e-02]],\n",
            "\n",
            "        [[ 7.2023e-02]],\n",
            "\n",
            "        [[-1.4712e-01]],\n",
            "\n",
            "        [[ 3.8262e-01]],\n",
            "\n",
            "        [[-8.2801e-02]],\n",
            "\n",
            "        [[-7.5763e-02]]], grad_fn=<AddBackward0>)\n",
            "answer tensor([[-1.2772],\n",
            "        [-0.7340],\n",
            "        [ 0.6295],\n",
            "        [-0.0411],\n",
            "        [-0.6029],\n",
            "        [-1.5769],\n",
            "        [ 0.2399],\n",
            "        [-0.8652],\n",
            "        [-0.8839],\n",
            "        [-0.3033],\n",
            "        [-1.1274],\n",
            "        [ 0.0339],\n",
            "        [ 2.3188],\n",
            "        [-0.5280],\n",
            "        [ 1.5697],\n",
            "        [-0.3969],\n",
            "        [-0.3782],\n",
            "        [-0.7902],\n",
            "        [-0.6779],\n",
            "        [-0.2284],\n",
            "        [-0.6029],\n",
            "        [ 1.4386],\n",
            "        [ 0.5208],\n",
            "        [-0.7153],\n",
            "        [-0.8090],\n",
            "        [ 0.1088],\n",
            "        [-0.1160],\n",
            "        [-0.8090],\n",
            "        [-0.8839],\n",
            "        [-0.1722],\n",
            "        [ 0.7175],\n",
            "        [-0.3407],\n",
            "        [ 0.0751],\n",
            "        [-0.0073],\n",
            "        [ 1.4198],\n",
            "        [ 0.5208],\n",
            "        [ 0.0713],\n",
            "        [-0.8764],\n",
            "        [-0.2284],\n",
            "        [-0.9401],\n",
            "        [-0.3595],\n",
            "        [-1.0899],\n",
            "        [-0.0785],\n",
            "        [-0.5318],\n",
            "        [-1.4270],\n",
            "        [ 1.9443],\n",
            "        [-0.0598],\n",
            "        [-0.7153],\n",
            "        [ 0.9965],\n",
            "        [-1.3521],\n",
            "        [-0.6029],\n",
            "        [-0.9775],\n",
            "        [ 0.4459],\n",
            "        [-0.1160],\n",
            "        [-0.6816],\n",
            "        [ 0.1837],\n",
            "        [ 0.8018],\n",
            "        [-0.1534],\n",
            "        [-0.1347],\n",
            "        [-0.8989],\n",
            "        [-0.1534],\n",
            "        [-0.3033],\n",
            "        [-1.2397],\n",
            "        [-1.3147],\n",
            "        [-0.3033],\n",
            "        [-0.9401],\n",
            "        [-0.1534],\n",
            "        [-0.2396],\n",
            "        [ 0.5583],\n",
            "        [-0.3033],\n",
            "        [-0.4531],\n",
            "        [ 0.5396],\n",
            "        [-0.6067],\n",
            "        [ 0.8205],\n",
            "        [-0.6779],\n",
            "        [ 0.3710],\n",
            "        [-0.7528],\n",
            "        [-1.2210],\n",
            "        [-0.3576],\n",
            "        [ 1.4198],\n",
            "        [-1.0337],\n",
            "        [-1.2023],\n",
            "        [-0.9026],\n",
            "        [-0.6217],\n",
            "        [ 0.0713],\n",
            "        [-1.6143],\n",
            "        [ 3.9858],\n",
            "        [-0.4156],\n",
            "        [-0.0411],\n",
            "        [ 1.7944],\n",
            "        [-1.6051],\n",
            "        [-1.0150],\n",
            "        [-1.0150],\n",
            "        [-1.1648],\n",
            "        [ 0.6332],\n",
            "        [ 0.3335],\n",
            "        [ 0.0713],\n",
            "        [ 1.4760],\n",
            "        [-0.3033],\n",
            "        [-1.0524],\n",
            "        [-1.1311],\n",
            "        [-0.4156],\n",
            "        [ 0.5957],\n",
            "        [-0.9588],\n",
            "        [ 1.9443],\n",
            "        [-1.5394],\n",
            "        [-0.1160],\n",
            "        [-0.6029],\n",
            "        [-0.4981]])\n",
            "hidden tensor([[-0.0143, -0.0101, -0.0131, -0.0198, -0.0088],\n",
            "        [-0.0015, -0.0013, -0.0016, -0.0024, -0.0008],\n",
            "        [-0.0305, -0.0125, -0.0304, -0.0431, -0.0144],\n",
            "        [-0.0323, -0.0274, -0.0377, -0.0500, -0.0243],\n",
            "        [-0.0147, -0.0129, -0.0142, -0.0214, -0.0101],\n",
            "        [-0.0181, -0.0129, -0.0147, -0.0223, -0.0129],\n",
            "        [-0.0076, -0.0060, -0.0087, -0.0115, -0.0057],\n",
            "        [-0.0386, -0.0297, -0.0377, -0.0609, -0.0270]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 1b\n",
        "\n",
        "seq_model = nn.Sequential(OrderedDict([\n",
        "    ('hidden_linear_1', nn.Linear(5, 8)),\n",
        "    ('hidden1_activation', nn.Tanh()),\n",
        "    ('hidden_linear_2', nn.Linear(8, 4)),\n",
        "    ('hidden2_activation', nn.Tanh()),\n",
        "    ('hidden_linear_3', nn.Linear(4, 2)),\n",
        "    ('hidden2_activation', nn.Tanh()),\n",
        "    ('output_linear', nn.Linear(2, 1))\n",
        "]))\n",
        "\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd1DJ4Gnej86",
        "outputId": "7db953ea-e3f0-45a9-d8eb-754575344254"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear_1): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden1_activation): Tanh()\n",
              "  (hidden_linear_2): Linear(in_features=8, out_features=4, bias=True)\n",
              "  (hidden2_activation): Tanh()\n",
              "  (hidden_linear_3): Linear(in_features=4, out_features=2, bias=True)\n",
              "  (output_linear): Linear(in_features=2, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voDPpsUTpB7S",
        "outputId": "e31552f3-a68f-4aea-f922-22ac673ccd1f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_linear_1.weight torch.Size([8, 5])\n",
            "hidden_linear_1.bias torch.Size([8])\n",
            "hidden_linear_2.weight torch.Size([4, 8])\n",
            "hidden_linear_2.bias torch.Size([4])\n",
            "hidden_linear_3.weight torch.Size([2, 4])\n",
            "hidden_linear_3.bias torch.Size([2])\n",
            "output_linear.weight torch.Size([1, 2])\n",
            "output_linear.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model.output_linear.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnd1Sd6jpChJ",
        "outputId": "b645d224-8d68-4498-d135-2999d72f6231"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.6456], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3) # <1>\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    X_Train = X_Train,\n",
        "    X_val = X_val, \n",
        "    y_Train = y_Train,\n",
        "    y_val = y_val)\n",
        "    \n",
        "print('output', seq_model(X_val))\n",
        "print('answer', y_val)\n",
        "#print('hidden', seq_model.hidden_linear.weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znFhxQXLpGXw",
        "outputId": "435b4a60-f24d-4680-d3bf-0f4848c16e67"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.4307, Validation loss 0.9913\n",
            "Epoch 20, Training loss 1.3799, Validation loss 0.9628\n",
            "Epoch 40, Training loss 1.3331, Validation loss 0.9381\n",
            "Epoch 60, Training loss 1.2923, Validation loss 0.9179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([436, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([109, 1])) that is different to the input size (torch.Size([436, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Training loss 1.2566, Validation loss 0.9015\n",
            "Epoch 100, Training loss 1.2255, Validation loss 0.8886\n",
            "Epoch 120, Training loss 1.1984, Validation loss 0.8784\n",
            "Epoch 140, Training loss 1.1747, Validation loss 0.8707\n",
            "Epoch 160, Training loss 1.1542, Validation loss 0.8650\n",
            "Epoch 180, Training loss 1.1363, Validation loss 0.8610\n",
            "Epoch 200, Training loss 1.1207, Validation loss 0.8585\n",
            "output tensor([[[-0.2800]],\n",
            "\n",
            "        [[-0.2504]],\n",
            "\n",
            "        [[-0.3205]],\n",
            "\n",
            "        [[-0.2590]],\n",
            "\n",
            "        [[-0.2547]],\n",
            "\n",
            "        [[-0.2288]],\n",
            "\n",
            "        [[-0.1700]],\n",
            "\n",
            "        [[-0.2522]],\n",
            "\n",
            "        [[-0.2989]],\n",
            "\n",
            "        [[-0.2333]],\n",
            "\n",
            "        [[-0.3096]],\n",
            "\n",
            "        [[-0.2745]],\n",
            "\n",
            "        [[-0.2509]],\n",
            "\n",
            "        [[-0.2827]],\n",
            "\n",
            "        [[-0.1676]],\n",
            "\n",
            "        [[-0.1741]],\n",
            "\n",
            "        [[-0.1996]],\n",
            "\n",
            "        [[-0.2735]],\n",
            "\n",
            "        [[-0.2166]],\n",
            "\n",
            "        [[-0.2561]],\n",
            "\n",
            "        [[-0.2094]],\n",
            "\n",
            "        [[-0.2045]],\n",
            "\n",
            "        [[-0.2636]],\n",
            "\n",
            "        [[-0.2861]],\n",
            "\n",
            "        [[-0.2855]],\n",
            "\n",
            "        [[-0.3412]],\n",
            "\n",
            "        [[-0.2596]],\n",
            "\n",
            "        [[-0.2902]],\n",
            "\n",
            "        [[-0.2618]],\n",
            "\n",
            "        [[-0.2840]],\n",
            "\n",
            "        [[-0.2218]],\n",
            "\n",
            "        [[-0.3289]],\n",
            "\n",
            "        [[-0.2386]],\n",
            "\n",
            "        [[-0.2391]],\n",
            "\n",
            "        [[-0.1945]],\n",
            "\n",
            "        [[-0.2126]],\n",
            "\n",
            "        [[-0.2644]],\n",
            "\n",
            "        [[-0.3245]],\n",
            "\n",
            "        [[-0.1718]],\n",
            "\n",
            "        [[-0.2064]],\n",
            "\n",
            "        [[-0.2273]],\n",
            "\n",
            "        [[-0.2365]],\n",
            "\n",
            "        [[-0.2334]],\n",
            "\n",
            "        [[-0.2849]],\n",
            "\n",
            "        [[-0.2468]],\n",
            "\n",
            "        [[-0.3232]],\n",
            "\n",
            "        [[-0.2607]],\n",
            "\n",
            "        [[-0.3080]],\n",
            "\n",
            "        [[-0.2573]],\n",
            "\n",
            "        [[-0.2877]],\n",
            "\n",
            "        [[-0.2837]],\n",
            "\n",
            "        [[-0.2544]],\n",
            "\n",
            "        [[-0.2995]],\n",
            "\n",
            "        [[-0.2773]],\n",
            "\n",
            "        [[-0.2730]],\n",
            "\n",
            "        [[-0.2225]],\n",
            "\n",
            "        [[-0.2661]],\n",
            "\n",
            "        [[-0.2137]],\n",
            "\n",
            "        [[-0.2195]],\n",
            "\n",
            "        [[-0.2896]],\n",
            "\n",
            "        [[-0.2373]],\n",
            "\n",
            "        [[-0.2573]],\n",
            "\n",
            "        [[-0.2520]],\n",
            "\n",
            "        [[-0.2793]],\n",
            "\n",
            "        [[-0.2612]],\n",
            "\n",
            "        [[-0.3206]],\n",
            "\n",
            "        [[-0.2369]],\n",
            "\n",
            "        [[-0.2863]],\n",
            "\n",
            "        [[-0.2340]],\n",
            "\n",
            "        [[-0.2524]],\n",
            "\n",
            "        [[-0.2753]],\n",
            "\n",
            "        [[-0.1793]],\n",
            "\n",
            "        [[-0.2510]],\n",
            "\n",
            "        [[-0.2387]],\n",
            "\n",
            "        [[-0.2633]],\n",
            "\n",
            "        [[-0.2598]],\n",
            "\n",
            "        [[-0.2283]],\n",
            "\n",
            "        [[-0.2677]],\n",
            "\n",
            "        [[-0.3054]],\n",
            "\n",
            "        [[-0.2918]],\n",
            "\n",
            "        [[-0.2475]],\n",
            "\n",
            "        [[-0.2347]],\n",
            "\n",
            "        [[-0.2444]],\n",
            "\n",
            "        [[-0.2902]],\n",
            "\n",
            "        [[-0.3347]],\n",
            "\n",
            "        [[-0.2979]],\n",
            "\n",
            "        [[-0.2016]],\n",
            "\n",
            "        [[-0.2331]],\n",
            "\n",
            "        [[-0.2635]],\n",
            "\n",
            "        [[-0.3008]],\n",
            "\n",
            "        [[-0.2925]],\n",
            "\n",
            "        [[-0.2781]],\n",
            "\n",
            "        [[-0.2830]],\n",
            "\n",
            "        [[-0.1901]],\n",
            "\n",
            "        [[-0.2951]],\n",
            "\n",
            "        [[-0.2263]],\n",
            "\n",
            "        [[-0.2769]],\n",
            "\n",
            "        [[-0.1599]],\n",
            "\n",
            "        [[-0.2270]],\n",
            "\n",
            "        [[-0.2232]],\n",
            "\n",
            "        [[-0.2793]],\n",
            "\n",
            "        [[-0.2620]],\n",
            "\n",
            "        [[-0.2095]],\n",
            "\n",
            "        [[-0.2608]],\n",
            "\n",
            "        [[-0.2434]],\n",
            "\n",
            "        [[-0.2401]],\n",
            "\n",
            "        [[-0.1928]],\n",
            "\n",
            "        [[-0.2347]],\n",
            "\n",
            "        [[-0.2565]]], grad_fn=<AddBackward0>)\n",
            "answer tensor([[-1.2772],\n",
            "        [-0.7340],\n",
            "        [ 0.6295],\n",
            "        [-0.0411],\n",
            "        [-0.6029],\n",
            "        [-1.5769],\n",
            "        [ 0.2399],\n",
            "        [-0.8652],\n",
            "        [-0.8839],\n",
            "        [-0.3033],\n",
            "        [-1.1274],\n",
            "        [ 0.0339],\n",
            "        [ 2.3188],\n",
            "        [-0.5280],\n",
            "        [ 1.5697],\n",
            "        [-0.3969],\n",
            "        [-0.3782],\n",
            "        [-0.7902],\n",
            "        [-0.6779],\n",
            "        [-0.2284],\n",
            "        [-0.6029],\n",
            "        [ 1.4386],\n",
            "        [ 0.5208],\n",
            "        [-0.7153],\n",
            "        [-0.8090],\n",
            "        [ 0.1088],\n",
            "        [-0.1160],\n",
            "        [-0.8090],\n",
            "        [-0.8839],\n",
            "        [-0.1722],\n",
            "        [ 0.7175],\n",
            "        [-0.3407],\n",
            "        [ 0.0751],\n",
            "        [-0.0073],\n",
            "        [ 1.4198],\n",
            "        [ 0.5208],\n",
            "        [ 0.0713],\n",
            "        [-0.8764],\n",
            "        [-0.2284],\n",
            "        [-0.9401],\n",
            "        [-0.3595],\n",
            "        [-1.0899],\n",
            "        [-0.0785],\n",
            "        [-0.5318],\n",
            "        [-1.4270],\n",
            "        [ 1.9443],\n",
            "        [-0.0598],\n",
            "        [-0.7153],\n",
            "        [ 0.9965],\n",
            "        [-1.3521],\n",
            "        [-0.6029],\n",
            "        [-0.9775],\n",
            "        [ 0.4459],\n",
            "        [-0.1160],\n",
            "        [-0.6816],\n",
            "        [ 0.1837],\n",
            "        [ 0.8018],\n",
            "        [-0.1534],\n",
            "        [-0.1347],\n",
            "        [-0.8989],\n",
            "        [-0.1534],\n",
            "        [-0.3033],\n",
            "        [-1.2397],\n",
            "        [-1.3147],\n",
            "        [-0.3033],\n",
            "        [-0.9401],\n",
            "        [-0.1534],\n",
            "        [-0.2396],\n",
            "        [ 0.5583],\n",
            "        [-0.3033],\n",
            "        [-0.4531],\n",
            "        [ 0.5396],\n",
            "        [-0.6067],\n",
            "        [ 0.8205],\n",
            "        [-0.6779],\n",
            "        [ 0.3710],\n",
            "        [-0.7528],\n",
            "        [-1.2210],\n",
            "        [-0.3576],\n",
            "        [ 1.4198],\n",
            "        [-1.0337],\n",
            "        [-1.2023],\n",
            "        [-0.9026],\n",
            "        [-0.6217],\n",
            "        [ 0.0713],\n",
            "        [-1.6143],\n",
            "        [ 3.9858],\n",
            "        [-0.4156],\n",
            "        [-0.0411],\n",
            "        [ 1.7944],\n",
            "        [-1.6051],\n",
            "        [-1.0150],\n",
            "        [-1.0150],\n",
            "        [-1.1648],\n",
            "        [ 0.6332],\n",
            "        [ 0.3335],\n",
            "        [ 0.0713],\n",
            "        [ 1.4760],\n",
            "        [-0.3033],\n",
            "        [-1.0524],\n",
            "        [-1.1311],\n",
            "        [-0.4156],\n",
            "        [ 0.5957],\n",
            "        [-0.9588],\n",
            "        [ 1.9443],\n",
            "        [-1.5394],\n",
            "        [-0.1160],\n",
            "        [-0.6029],\n",
            "        [-0.4981]])\n"
          ]
        }
      ]
    }
  ]
}