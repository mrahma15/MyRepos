{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrahma15/MyRepos/blob/main/Homework_2_Problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Qb1OMPW_MY",
        "outputId": "0250cc5d-a3db-40fb-d9da-970d3d8fdcd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1a7cebc390>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "75cxTfjka7dR"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_ANnsSW8a8B0"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "data_path = '/content/drive/MyDrive/Google Colab data'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R2069rVAbAIV"
      },
      "outputs": [],
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VSpZeT4fcMT5"
      },
      "outputs": [],
      "source": [
        "output_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8MQ778SbeM4T"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                3072,  # <1>\n",
        "                512,   # <2>\n",
        "            ),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(\n",
        "                512,   # <2>\n",
        "                output_size, # <3>\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LdtrxoPmvuih"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZUmZtOs7vyG4"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, output_size),\n",
        "            nn.Softmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "6BmEEqwLwKhl",
        "outputId": "71cd9298-7352-490e-9bf5-2df12d3c374d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "img, _ = cifar10[0]\n",
        "\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kUnhpiXAxp2o"
      },
      "outputs": [],
      "source": [
        "img_batch = img.view(-1).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyK10gmXx2UF",
        "outputId": "d8241a4a-dea3-4de0-ff03-fb9a8bb70ac2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0998, 0.0868, 0.1027, 0.0979, 0.0797, 0.1334, 0.0710, 0.0899, 0.1487,\n",
              "         0.0902]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "out = model(img_batch)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "77MdE-dWx_xy"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "nn.Linear(3072, 512),\n",
        "nn.Tanh(),\n",
        "nn.Linear(512, output_size),\n",
        "nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KJjNYyfezH_f"
      },
      "outputs": [],
      "source": [
        "loss = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oLJ0uoCo1cyB"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#             nn.Linear(3072, 512),\n",
        "#             nn.Tanh(),\n",
        "#             nn.Linear(512, output_size),\n",
        "#             nn.LogSoftmax(dim=1))\n",
        "\n",
        "# learning_rate = 1e-2\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# loss_fn = nn.NLLLoss()\n",
        "\n",
        "# n_epochs = 200\n",
        "\n",
        "# for epoch in range(n_epochs):\n",
        "#     for img, label in cifar10:\n",
        "#         out = model(img.view(-1).unsqueeze(0))\n",
        "#         loss = loss_fn(out, torch.tensor([label]))\n",
        "                \n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XnzI0HaM9LPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fdb4a8b-a26d-48fd-b97e-2d09721c7e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.550965\n",
            "Epoch: 1, Loss: 1.962476\n",
            "Epoch: 2, Loss: 1.663115\n",
            "Epoch: 3, Loss: 1.717850\n",
            "Epoch: 4, Loss: 1.869302\n",
            "Epoch: 5, Loss: 1.657579\n",
            "Epoch: 6, Loss: 0.864403\n",
            "Epoch: 7, Loss: 1.004602\n",
            "Epoch: 8, Loss: 1.336312\n",
            "Epoch: 9, Loss: 1.699206\n",
            "Epoch: 10, Loss: 1.403259\n",
            "Epoch: 11, Loss: 1.703896\n",
            "Epoch: 12, Loss: 1.053710\n",
            "Epoch: 13, Loss: 0.967323\n",
            "Epoch: 14, Loss: 1.410554\n",
            "Epoch: 15, Loss: 0.883521\n",
            "Epoch: 16, Loss: 0.933655\n",
            "Epoch: 17, Loss: 1.455976\n",
            "Epoch: 18, Loss: 0.843374\n",
            "Epoch: 19, Loss: 1.043579\n",
            "Epoch: 20, Loss: 0.958298\n",
            "Epoch: 21, Loss: 1.202510\n",
            "Epoch: 22, Loss: 1.053872\n",
            "Epoch: 23, Loss: 0.732621\n",
            "Epoch: 24, Loss: 0.825837\n",
            "Epoch: 25, Loss: 0.667394\n",
            "Epoch: 26, Loss: 0.591164\n",
            "Epoch: 27, Loss: 0.927300\n",
            "Epoch: 28, Loss: 0.530024\n",
            "Epoch: 29, Loss: 0.615687\n",
            "Epoch: 30, Loss: 0.398452\n",
            "Epoch: 31, Loss: 0.719917\n",
            "Epoch: 32, Loss: 0.670533\n",
            "Epoch: 33, Loss: 0.684978\n",
            "Epoch: 34, Loss: 0.470533\n",
            "Epoch: 35, Loss: 0.390720\n",
            "Epoch: 36, Loss: 0.438041\n",
            "Epoch: 37, Loss: 0.589412\n",
            "Epoch: 38, Loss: 0.726305\n",
            "Epoch: 39, Loss: 0.572858\n",
            "Epoch: 40, Loss: 0.615268\n",
            "Epoch: 41, Loss: 0.667243\n",
            "Epoch: 42, Loss: 0.478859\n",
            "Epoch: 43, Loss: 0.309648\n",
            "Epoch: 44, Loss: 0.260996\n",
            "Epoch: 45, Loss: 0.492661\n",
            "Epoch: 46, Loss: 0.239429\n",
            "Epoch: 47, Loss: 0.382060\n",
            "Epoch: 48, Loss: 0.217544\n",
            "Epoch: 49, Loss: 0.571312\n",
            "Epoch: 50, Loss: 0.418731\n",
            "Epoch: 51, Loss: 0.181591\n",
            "Epoch: 52, Loss: 0.359125\n",
            "Epoch: 53, Loss: 0.239674\n",
            "Epoch: 54, Loss: 0.308591\n",
            "Epoch: 55, Loss: 0.162381\n",
            "Epoch: 56, Loss: 0.274236\n",
            "Epoch: 57, Loss: 0.355163\n",
            "Epoch: 58, Loss: 0.258132\n",
            "Epoch: 59, Loss: 0.256401\n",
            "Epoch: 60, Loss: 0.376096\n",
            "Epoch: 61, Loss: 0.055303\n",
            "Epoch: 62, Loss: 0.222062\n",
            "Epoch: 63, Loss: 0.120957\n",
            "Epoch: 64, Loss: 0.221185\n",
            "Epoch: 65, Loss: 0.086143\n",
            "Epoch: 66, Loss: 0.069654\n",
            "Epoch: 67, Loss: 0.086102\n",
            "Epoch: 68, Loss: 0.100348\n",
            "Epoch: 69, Loss: 0.076734\n",
            "Epoch: 70, Loss: 0.155775\n",
            "Epoch: 71, Loss: 0.066432\n",
            "Epoch: 72, Loss: 0.159088\n",
            "Epoch: 73, Loss: 0.076625\n",
            "Epoch: 74, Loss: 0.303359\n",
            "Epoch: 75, Loss: 0.108969\n",
            "Epoch: 76, Loss: 0.145393\n",
            "Epoch: 77, Loss: 0.078614\n",
            "Epoch: 78, Loss: 0.073971\n",
            "Epoch: 79, Loss: 0.120809\n",
            "Epoch: 80, Loss: 0.117727\n",
            "Epoch: 81, Loss: 0.108850\n",
            "Epoch: 82, Loss: 0.037895\n",
            "Epoch: 83, Loss: 0.067016\n",
            "Epoch: 84, Loss: 0.059898\n",
            "Epoch: 85, Loss: 0.079099\n",
            "Epoch: 86, Loss: 0.059256\n",
            "Epoch: 87, Loss: 0.064529\n",
            "Epoch: 88, Loss: 0.062785\n",
            "Epoch: 89, Loss: 0.088598\n",
            "Epoch: 90, Loss: 0.065729\n",
            "Epoch: 91, Loss: 0.041477\n",
            "Epoch: 92, Loss: 0.033366\n",
            "Epoch: 93, Loss: 0.088857\n",
            "Epoch: 94, Loss: 0.076050\n",
            "Epoch: 95, Loss: 0.049875\n",
            "Epoch: 96, Loss: 0.066792\n",
            "Epoch: 97, Loss: 0.030015\n",
            "Epoch: 98, Loss: 0.107473\n",
            "Epoch: 99, Loss: 0.038680\n",
            "Epoch: 100, Loss: 0.031779\n",
            "Epoch: 101, Loss: 0.032356\n",
            "Epoch: 102, Loss: 0.053710\n",
            "Epoch: 103, Loss: 0.052024\n",
            "Epoch: 104, Loss: 0.012788\n",
            "Epoch: 105, Loss: 0.036520\n",
            "Epoch: 106, Loss: 0.106747\n",
            "Epoch: 107, Loss: 0.032758\n",
            "Epoch: 108, Loss: 0.032339\n",
            "Epoch: 109, Loss: 0.028122\n",
            "Epoch: 110, Loss: 0.022442\n",
            "Epoch: 111, Loss: 0.033670\n",
            "Epoch: 112, Loss: 0.035735\n",
            "Epoch: 113, Loss: 0.037099\n",
            "Epoch: 114, Loss: 0.031391\n",
            "Epoch: 115, Loss: 0.031151\n",
            "Epoch: 116, Loss: 0.024034\n",
            "Epoch: 117, Loss: 0.027423\n",
            "Epoch: 118, Loss: 0.017822\n",
            "Epoch: 119, Loss: 0.027722\n",
            "Epoch: 120, Loss: 0.024018\n",
            "Epoch: 121, Loss: 0.034642\n",
            "Epoch: 122, Loss: 0.021036\n",
            "Epoch: 123, Loss: 0.024995\n",
            "Epoch: 124, Loss: 0.016753\n",
            "Epoch: 125, Loss: 0.016024\n",
            "Epoch: 126, Loss: 0.021362\n",
            "Epoch: 127, Loss: 0.014904\n",
            "Epoch: 128, Loss: 0.030081\n",
            "Epoch: 129, Loss: 0.025703\n",
            "Epoch: 130, Loss: 0.032475\n",
            "Epoch: 131, Loss: 0.022639\n",
            "Epoch: 132, Loss: 0.017714\n",
            "Epoch: 133, Loss: 0.013869\n",
            "Epoch: 134, Loss: 0.031623\n",
            "Epoch: 135, Loss: 0.037437\n",
            "Epoch: 136, Loss: 0.013567\n",
            "Epoch: 137, Loss: 0.011775\n",
            "Epoch: 138, Loss: 0.013697\n",
            "Epoch: 139, Loss: 0.015369\n",
            "Epoch: 140, Loss: 0.060359\n",
            "Epoch: 141, Loss: 0.017138\n",
            "Epoch: 142, Loss: 0.015375\n",
            "Epoch: 143, Loss: 0.017417\n",
            "Epoch: 144, Loss: 0.028005\n",
            "Epoch: 145, Loss: 0.055070\n",
            "Epoch: 146, Loss: 0.016818\n",
            "Epoch: 147, Loss: 0.014399\n",
            "Epoch: 148, Loss: 0.017951\n",
            "Epoch: 149, Loss: 0.014857\n",
            "Epoch: 150, Loss: 0.017446\n",
            "Epoch: 151, Loss: 0.018009\n",
            "Epoch: 152, Loss: 0.018268\n",
            "Epoch: 153, Loss: 0.021465\n",
            "Epoch: 154, Loss: 0.015592\n",
            "Epoch: 155, Loss: 0.015394\n",
            "Epoch: 156, Loss: 0.014139\n",
            "Epoch: 157, Loss: 0.018434\n",
            "Epoch: 158, Loss: 0.010551\n",
            "Epoch: 159, Loss: 0.017326\n",
            "Epoch: 160, Loss: 0.019976\n",
            "Epoch: 161, Loss: 0.015595\n",
            "Epoch: 162, Loss: 0.019457\n",
            "Epoch: 163, Loss: 0.019035\n",
            "Epoch: 164, Loss: 0.013344\n",
            "Epoch: 165, Loss: 0.013157\n",
            "Epoch: 166, Loss: 0.031038\n",
            "Epoch: 167, Loss: 0.016308\n",
            "Epoch: 168, Loss: 0.013823\n",
            "Epoch: 169, Loss: 0.020358\n",
            "Epoch: 170, Loss: 0.014463\n",
            "Epoch: 171, Loss: 0.005517\n",
            "Epoch: 172, Loss: 0.011948\n",
            "Epoch: 173, Loss: 0.021774\n",
            "Epoch: 174, Loss: 0.009463\n",
            "Epoch: 175, Loss: 0.041756\n",
            "Epoch: 176, Loss: 0.013796\n",
            "Epoch: 177, Loss: 0.016039\n",
            "Epoch: 178, Loss: 0.013597\n",
            "Epoch: 179, Loss: 0.011557\n",
            "Epoch: 180, Loss: 0.014998\n",
            "Epoch: 181, Loss: 0.018388\n",
            "Epoch: 182, Loss: 0.009898\n",
            "Epoch: 183, Loss: 0.019402\n",
            "Epoch: 184, Loss: 0.013809\n",
            "Epoch: 185, Loss: 0.009757\n",
            "Epoch: 186, Loss: 0.013700\n",
            "Epoch: 187, Loss: 0.013661\n",
            "Epoch: 188, Loss: 0.014220\n",
            "Epoch: 189, Loss: 0.009272\n",
            "Epoch: 190, Loss: 0.009709\n",
            "Epoch: 191, Loss: 0.015453\n",
            "Epoch: 192, Loss: 0.012916\n",
            "Epoch: 193, Loss: 0.014325\n",
            "Epoch: 194, Loss: 0.013846\n",
            "Epoch: 195, Loss: 0.019039\n",
            "Epoch: 196, Loss: 0.019323\n",
            "Epoch: 197, Loss: 0.010828\n",
            "Epoch: 198, Loss: 0.011350\n",
            "Epoch: 199, Loss: 0.009609\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, output_size),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 200\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bm0yZp0U72_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc6cd9b-e727-4ec0-f172-22ba2660414f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4QIcKqEb8Adl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ebab12-fb2d-4d14-9795-7f9af30f864a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.462600\n"
          ]
        }
      ],
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6OBO8FTV8Duq"
      },
      "outputs": [],
      "source": [
        "#Problem 2b\n",
        "#Not yet done\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 10))\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 10))\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 200\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW9OJyDiVT4J",
        "outputId": "ae6b7c9d-4461-4d27-d67b-7f5d9f6230ba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.026180\n",
            "Epoch: 1, Loss: 1.734692\n",
            "Epoch: 2, Loss: 2.029119\n",
            "Epoch: 3, Loss: 1.643143\n",
            "Epoch: 4, Loss: 1.686933\n",
            "Epoch: 5, Loss: 1.504715\n",
            "Epoch: 6, Loss: 1.401178\n",
            "Epoch: 7, Loss: 1.668288\n",
            "Epoch: 8, Loss: 1.511410\n",
            "Epoch: 9, Loss: 1.041813\n",
            "Epoch: 10, Loss: 1.150139\n",
            "Epoch: 11, Loss: 0.875672\n",
            "Epoch: 12, Loss: 1.073029\n",
            "Epoch: 13, Loss: 1.505690\n",
            "Epoch: 14, Loss: 1.206806\n",
            "Epoch: 15, Loss: 1.382367\n",
            "Epoch: 16, Loss: 0.731889\n",
            "Epoch: 17, Loss: 0.990599\n",
            "Epoch: 18, Loss: 1.207527\n",
            "Epoch: 19, Loss: 0.995946\n",
            "Epoch: 20, Loss: 0.727798\n",
            "Epoch: 21, Loss: 0.958559\n",
            "Epoch: 22, Loss: 0.749253\n",
            "Epoch: 23, Loss: 1.238220\n",
            "Epoch: 24, Loss: 0.965578\n",
            "Epoch: 25, Loss: 0.926038\n",
            "Epoch: 26, Loss: 0.636256\n",
            "Epoch: 27, Loss: 0.821756\n",
            "Epoch: 28, Loss: 0.769000\n",
            "Epoch: 29, Loss: 0.891552\n",
            "Epoch: 30, Loss: 0.424909\n",
            "Epoch: 31, Loss: 0.187856\n",
            "Epoch: 32, Loss: 0.287391\n",
            "Epoch: 33, Loss: 0.495762\n",
            "Epoch: 34, Loss: 0.500348\n",
            "Epoch: 35, Loss: 0.329413\n",
            "Epoch: 36, Loss: 0.666851\n",
            "Epoch: 37, Loss: 0.514406\n",
            "Epoch: 38, Loss: 0.181881\n",
            "Epoch: 39, Loss: 0.222370\n",
            "Epoch: 40, Loss: 0.243326\n",
            "Epoch: 41, Loss: 0.818511\n",
            "Epoch: 42, Loss: 0.120952\n",
            "Epoch: 43, Loss: 0.392770\n",
            "Epoch: 44, Loss: 0.175660\n",
            "Epoch: 45, Loss: 0.247080\n",
            "Epoch: 46, Loss: 0.583056\n",
            "Epoch: 47, Loss: 0.214728\n",
            "Epoch: 48, Loss: 0.042108\n",
            "Epoch: 49, Loss: 0.152265\n",
            "Epoch: 50, Loss: 0.126441\n",
            "Epoch: 51, Loss: 0.050106\n",
            "Epoch: 52, Loss: 0.214249\n",
            "Epoch: 53, Loss: 0.029620\n",
            "Epoch: 54, Loss: 0.017599\n",
            "Epoch: 55, Loss: 0.039198\n",
            "Epoch: 56, Loss: 0.025609\n",
            "Epoch: 57, Loss: 0.069744\n",
            "Epoch: 58, Loss: 0.018446\n",
            "Epoch: 59, Loss: 0.014470\n",
            "Epoch: 60, Loss: 0.017292\n",
            "Epoch: 61, Loss: 0.015932\n",
            "Epoch: 62, Loss: 0.013253\n",
            "Epoch: 63, Loss: 0.087329\n",
            "Epoch: 64, Loss: 0.016545\n",
            "Epoch: 65, Loss: 0.450602\n",
            "Epoch: 66, Loss: 0.085944\n",
            "Epoch: 67, Loss: 0.016064\n",
            "Epoch: 68, Loss: 0.024578\n",
            "Epoch: 69, Loss: 0.041536\n",
            "Epoch: 70, Loss: 0.006012\n",
            "Epoch: 71, Loss: 0.002883\n",
            "Epoch: 72, Loss: 0.004741\n",
            "Epoch: 73, Loss: 0.001593\n",
            "Epoch: 74, Loss: 0.006730\n",
            "Epoch: 75, Loss: 0.004023\n",
            "Epoch: 76, Loss: 0.001785\n",
            "Epoch: 77, Loss: 0.003961\n",
            "Epoch: 78, Loss: 0.001704\n",
            "Epoch: 79, Loss: 0.003765\n",
            "Epoch: 80, Loss: 0.005130\n",
            "Epoch: 81, Loss: 0.003281\n",
            "Epoch: 82, Loss: 0.001696\n",
            "Epoch: 83, Loss: 0.005608\n",
            "Epoch: 84, Loss: 0.002422\n",
            "Epoch: 85, Loss: 0.001863\n",
            "Epoch: 86, Loss: 0.002620\n",
            "Epoch: 87, Loss: 0.003666\n",
            "Epoch: 88, Loss: 0.002966\n",
            "Epoch: 89, Loss: 0.002501\n",
            "Epoch: 90, Loss: 0.002432\n",
            "Epoch: 91, Loss: 0.002834\n",
            "Epoch: 92, Loss: 0.001271\n",
            "Epoch: 93, Loss: 0.001788\n",
            "Epoch: 94, Loss: 0.001451\n",
            "Epoch: 95, Loss: 0.003314\n",
            "Epoch: 96, Loss: 0.001100\n",
            "Epoch: 97, Loss: 0.001569\n",
            "Epoch: 98, Loss: 0.001388\n",
            "Epoch: 99, Loss: 0.000820\n",
            "Epoch: 100, Loss: 0.002006\n",
            "Epoch: 101, Loss: 0.002607\n",
            "Epoch: 102, Loss: 0.002718\n",
            "Epoch: 103, Loss: 0.001407\n",
            "Epoch: 104, Loss: 0.001887\n",
            "Epoch: 105, Loss: 0.001073\n",
            "Epoch: 106, Loss: 0.002118\n",
            "Epoch: 107, Loss: 0.001040\n",
            "Epoch: 108, Loss: 0.002776\n",
            "Epoch: 109, Loss: 0.002354\n",
            "Epoch: 110, Loss: 0.001726\n",
            "Epoch: 111, Loss: 0.001967\n",
            "Epoch: 112, Loss: 0.000906\n",
            "Epoch: 113, Loss: 0.001390\n",
            "Epoch: 114, Loss: 0.001312\n",
            "Epoch: 115, Loss: 0.001543\n",
            "Epoch: 116, Loss: 0.001100\n",
            "Epoch: 117, Loss: 0.001478\n",
            "Epoch: 118, Loss: 0.000769\n",
            "Epoch: 119, Loss: 0.000648\n",
            "Epoch: 120, Loss: 0.000726\n",
            "Epoch: 121, Loss: 0.001275\n",
            "Epoch: 122, Loss: 0.001020\n",
            "Epoch: 123, Loss: 0.001303\n",
            "Epoch: 124, Loss: 0.001222\n",
            "Epoch: 125, Loss: 0.000766\n",
            "Epoch: 126, Loss: 0.000997\n",
            "Epoch: 127, Loss: 0.001164\n",
            "Epoch: 128, Loss: 0.000902\n",
            "Epoch: 129, Loss: 0.000826\n",
            "Epoch: 130, Loss: 0.001660\n",
            "Epoch: 131, Loss: 0.001282\n",
            "Epoch: 132, Loss: 0.000502\n",
            "Epoch: 133, Loss: 0.001626\n",
            "Epoch: 134, Loss: 0.001038\n",
            "Epoch: 135, Loss: 0.000772\n",
            "Epoch: 136, Loss: 0.001198\n",
            "Epoch: 137, Loss: 0.000836\n",
            "Epoch: 138, Loss: 0.001037\n",
            "Epoch: 139, Loss: 0.000610\n",
            "Epoch: 140, Loss: 0.001366\n",
            "Epoch: 141, Loss: 0.001988\n",
            "Epoch: 142, Loss: 0.000818\n",
            "Epoch: 143, Loss: 0.001165\n",
            "Epoch: 144, Loss: 0.001353\n",
            "Epoch: 145, Loss: 0.000936\n",
            "Epoch: 146, Loss: 0.000667\n",
            "Epoch: 147, Loss: 0.000353\n",
            "Epoch: 148, Loss: 0.001034\n",
            "Epoch: 149, Loss: 0.000882\n",
            "Epoch: 150, Loss: 0.000589\n",
            "Epoch: 151, Loss: 0.001175\n",
            "Epoch: 152, Loss: 0.001237\n",
            "Epoch: 153, Loss: 0.000676\n",
            "Epoch: 154, Loss: 0.000847\n",
            "Epoch: 155, Loss: 0.000957\n",
            "Epoch: 156, Loss: 0.000860\n",
            "Epoch: 157, Loss: 0.001019\n",
            "Epoch: 158, Loss: 0.001246\n",
            "Epoch: 159, Loss: 0.000814\n",
            "Epoch: 160, Loss: 0.001365\n",
            "Epoch: 161, Loss: 0.001013\n",
            "Epoch: 162, Loss: 0.001188\n",
            "Epoch: 163, Loss: 0.000609\n",
            "Epoch: 164, Loss: 0.000717\n",
            "Epoch: 165, Loss: 0.000594\n",
            "Epoch: 166, Loss: 0.001004\n",
            "Epoch: 167, Loss: 0.000774\n",
            "Epoch: 168, Loss: 0.000392\n",
            "Epoch: 169, Loss: 0.000557\n",
            "Epoch: 170, Loss: 0.000782\n",
            "Epoch: 171, Loss: 0.000543\n",
            "Epoch: 172, Loss: 0.001074\n",
            "Epoch: 173, Loss: 0.000479\n",
            "Epoch: 174, Loss: 0.001309\n",
            "Epoch: 175, Loss: 0.000444\n",
            "Epoch: 176, Loss: 0.000654\n",
            "Epoch: 177, Loss: 0.000548\n",
            "Epoch: 178, Loss: 0.000764\n",
            "Epoch: 179, Loss: 0.000484\n",
            "Epoch: 180, Loss: 0.000672\n",
            "Epoch: 181, Loss: 0.000645\n",
            "Epoch: 182, Loss: 0.000753\n",
            "Epoch: 183, Loss: 0.000391\n",
            "Epoch: 184, Loss: 0.001322\n",
            "Epoch: 185, Loss: 0.000708\n",
            "Epoch: 186, Loss: 0.000580\n",
            "Epoch: 187, Loss: 0.000584\n",
            "Epoch: 188, Loss: 0.000574\n",
            "Epoch: 189, Loss: 0.000565\n",
            "Epoch: 190, Loss: 0.000607\n",
            "Epoch: 191, Loss: 0.000412\n",
            "Epoch: 192, Loss: 0.000420\n",
            "Epoch: 193, Loss: 0.000366\n",
            "Epoch: 194, Loss: 0.000573\n",
            "Epoch: 195, Loss: 0.001321\n",
            "Epoch: 196, Loss: 0.000713\n",
            "Epoch: 197, Loss: 0.000373\n",
            "Epoch: 198, Loss: 0.000512\n",
            "Epoch: 199, Loss: 0.000444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc1A_9_-VX0l",
        "outputId": "1aee4d53-1c95-41a8-e41e-6b54a4b41b17"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHE2PGBlVbWO",
        "outputId": "530896a5-1d93-462a-cbcd-3cd4311f35c0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.460800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rlNEGDF3UkX1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tYAszMRJVDK-"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Homework_2_Problem_2a.ipynb",
      "provenance": [],
      "mount_file_id": "1A0Mt1EHzivaH-6GyKxYfZKX9wirx26WQ",
      "authorship_tag": "ABX9TyPA8UAOklrgLCrp1YcjPYzW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}